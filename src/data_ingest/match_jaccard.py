import json  # å°å…¥ json æ¨¡çµ„ï¼Œç”¨æ–¼è™•ç† JSON æ ¼å¼çš„è³‡æ–™ã€‚
import re  # å°å…¥ re æ¨¡çµ„ï¼Œç”¨æ–¼è™•ç†æ­£è¦è¡¨é”å¼ï¼Œé€²è¡Œå­—ä¸²åŒ¹é…å’Œè™•ç†ã€‚
from pathlib import Path  # è™•ç†æª”æ¡ˆè·¯å¾‘ã€‚
from tqdm import tqdm  # é¡¯ç¤ºé€²åº¦æ¢ã€‚
import torch  # PyTorchï¼Œç”¨æ–¼æ·±åº¦å­¸ç¿’å’Œ GPU åŠ é€Ÿã€‚
from ckip_transformers.nlp import CkipWordSegmenter  # CKIP æ–·è©å™¨ã€‚
import gc  # åƒåœ¾å›æ”¶ã€‚
from torch.utils.data import DataLoader, TensorDataset # å°å…¥ PyTorch DataLoaderï¼Œç”¨æ–¼æ›´æ–¹ä¾¿åœ°è™•ç†è³‡æ–™æ‰¹æ¬¡ã€‚

# ========== ä½¿ç”¨è€…è¨­å®š ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")  # å®šç¾©å·²è™•ç†çš„ JSON æª”æ¡ˆè·¯å¾‘ï¼Œè©²æª”æ¡ˆåŒ…å«åŸå§‹å¥å­ã€‚
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/compared_text")  # å®šç¾©åŒ…å«å¾…æ¯”å°æ–‡æœ¬æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ã€‚
OUTPUT_JSON_PATH = Path(r"D:/lufu_allusion/data/processed/sample_match_results_jaccard_gpu.json")  # å®šç¾©è¼¸å‡º JSON æª”æ¡ˆçš„è·¯å¾‘ï¼Œè©²æª”æ¡ˆå°‡åŒ…å«æ¯”å°çµæœã€‚
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\-\\ï¼\\(\\)\\[\\]/(),1234567890Â¶"  # å®šç¾©éœ€è¦å¾æ–‡æœ¬ä¸­ç§»é™¤çš„å­—å…ƒã€‚
JACCARD_THRESHOLD = 0.45  # å®šç¾© Jaccard ç›¸ä¼¼åº¦é–¾å€¼ï¼Œåªæœ‰è¶…éæ­¤é–¾å€¼çš„å¥å­æ‰è¢«è¦–ç‚ºåŒ¹é…ã€‚

# ========== è¨­å‚™æª¢æ¸¬ ==========
if torch.cuda.is_available():  # æª¢æŸ¥ç³»çµ±ä¸­æ˜¯å¦æœ‰å¯ç”¨çš„ GPUã€‚
    DEVICE = torch.device("cuda")  # å¦‚æœæœ‰ GPUï¼Œå‰‡å°‡è£ç½®è¨­å®šç‚º GPUã€‚
    print(f"âœ… åµæ¸¬åˆ° GPU: {torch.cuda.get_device_name(0)}ï¼Œå°‡ä½¿ç”¨ GPU åŠ é€Ÿï¼")  # å°å‡ºå·²åµæ¸¬åˆ° GPU çš„è¨Šæ¯ã€‚
    CKIP_DEVICE = 0  # è¨­å®š CKIP æ–·è©å™¨ä½¿ç”¨ GPUã€‚
else:
    DEVICE = torch.device("cpu")  # å¦‚æœæ²’æœ‰ GPUï¼Œå‰‡å°‡è£ç½®è¨­å®šç‚º CPUã€‚
    print("âš ï¸ æ²’æœ‰åµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU é‹ç®—ã€‚")  # å°å‡ºæœªä½¿ç”¨ GPU çš„è¨Šæ¯ã€‚
    CKIP_DEVICE = -1  # è¨­å®š CKIP æ–·è©å™¨ä½¿ç”¨ CPUã€‚

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [...]  # å®šç¾©éœ€è¦æ’é™¤çš„å‰ç¶´è©åˆ—è¡¨ï¼ˆç›®å‰ç‚ºç©ºï¼Œå¯æ ¹æ“šéœ€è¦æ·»åŠ ï¼‰ã€‚
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰"]  # å®šç¾©éœ€è¦æ’é™¤çš„å¾Œç¶´è©åˆ—è¡¨ã€‚

def clean_sentence(text):
    """
    æ¸…ç†å¥å­ä¸­çš„å‰å¾Œç¶´å’Œç©ºç™½å­—å…ƒã€‚

    Args:
        text (str): éœ€è¦æ¸…ç†çš„å¥å­ã€‚

    Returns:
        str: æ¸…ç†å¾Œçš„å¥å­ã€‚
    """
    # æ¸…ç†å‰å¾Œç¶´
    for prefix in PREFIX_EXCLUDE:  # è¿­ä»£è™•ç†æ‰€æœ‰éœ€è¦æ’é™¤çš„å‰ç¶´è©ã€‚
        if isinstance(text, str) and text.startswith(prefix):  # æª¢æŸ¥å¥å­æ˜¯å¦ä»¥æŒ‡å®šå‰ç¶´è©é–‹å§‹ã€‚
            text = text[len(prefix):]  # å¦‚æœæ˜¯ï¼Œå‰‡ç§»é™¤è©²å‰ç¶´è©ã€‚
            break  # ç§»é™¤å‰ç¶´è©å¾Œï¼Œè·³å‡ºè¿´åœˆï¼Œä¸å†æª¢æŸ¥å…¶ä»–å‰ç¶´è©ã€‚
    for suffix in SUFFIX_EXCLUDE:  # è¿­ä»£è™•ç†æ‰€æœ‰éœ€è¦æ’é™¤çš„å¾Œç¶´è©ã€‚
        if isinstance(text, str) and text.endswith(suffix):  # æª¢æŸ¥å¥å­æ˜¯å¦ä»¥æŒ‡å®šå¾Œç¶´è©çµæŸã€‚
            text = text[:-len(suffix)]  # å¦‚æœæ˜¯ï¼Œå‰‡ç§»é™¤è©²å¾Œç¶´è©ã€‚
            break  # ç§»é™¤å¾Œç¶´è©å¾Œï¼Œè·³å‡ºè¿´åœˆï¼Œä¸å†æª¢æŸ¥å…¶ä»–å¾Œç¶´è©ã€‚
    return text.strip()  # ç§»é™¤å¥å­é¦–å°¾çš„ç©ºç™½å­—å…ƒï¼Œä¸¦è¿”å›æ¸…ç†å¾Œçš„å¥å­ã€‚

# ========== è³‡æ–™è¼‰å…¥èˆ‡æ¸…æ´— ==========
def load_parsed_results(json_path):
    """
    å¾ JSON æª”æ¡ˆä¸­è¼‰å…¥å·²è™•ç†çš„åŸå§‹å¥å­ã€‚

    Args:
        json_path (Path): JSON æª”æ¡ˆçš„è·¯å¾‘ã€‚

    Returns:
        list: åŒ…å«åŸå§‹å¥å­çš„åˆ—è¡¨ã€‚
    """
    print("ğŸ”„ï¸ æ­£åœ¨è¼‰å…¥åŸå¥è³‡æ–™...")  # å°å‡ºè¼‰å…¥è³‡æ–™çš„è¨Šæ¯ã€‚
    with open(json_path, encoding="utf-8") as f:  # ä»¥ UTF-8 ç·¨ç¢¼é–‹å•Ÿ JSON æª”æ¡ˆã€‚
        data = json.load(f)  # å°‡ JSON æª”æ¡ˆçš„å…§å®¹è¼‰å…¥åˆ° Python ç‰©ä»¶ä¸­ã€‚
    records = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜åŸå§‹å¥å­ã€‚
    for article in data:  # è¿­ä»£è™•ç† JSON è³‡æ–™ä¸­çš„æ¯å€‹æ–‡ç« ã€‚
        for para in article.get("æ®µè½", []):  # è¿­ä»£è™•ç†æ–‡ç« ä¸­çš„æ¯å€‹æ®µè½ã€‚
            for group in para.get("å¥çµ„", []):  # è¿­ä»£è™•ç†æ®µè½ä¸­çš„æ¯å€‹å¥çµ„ã€‚
                for sent in group.get("å¥å­", []):  # è¿­ä»£è™•ç†å¥çµ„ä¸­çš„æ¯å€‹å¥å­ã€‚
                    content = sent.get("å…§å®¹", "")  # ç²å–å¥å­çš„å…§å®¹ã€‚
                    records.append(clean_sentence(content))  # æ¸…ç†å¥å­ä¸¦å°‡å…¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} å¥åŸæ–‡å¥å­ã€‚")  # å°å‡ºè³‡æ–™è¼‰å…¥å®Œæˆçš„è¨Šæ¯ï¼Œä¸¦é¡¯ç¤ºè¼‰å…¥çš„å¥å­æ•¸é‡ã€‚
    return records  # è¿”å›åŒ…å«åŸå§‹å¥å­çš„åˆ—è¡¨ã€‚

def load_compared_sentences(folder_path, chars_to_remove):
    """
    å¾æŒ‡å®šè³‡æ–™å¤¾ä¸­è¼‰å…¥å¾…æ¯”å°çš„å¥å­ï¼Œä¸¦ç§»é™¤æŒ‡å®šçš„å­—å…ƒã€‚

    Args:
        folder_path (Path): åŒ…å«å¾…æ¯”å°æ–‡æœ¬æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ã€‚
        chars_to_remove (str): éœ€è¦å¾æ–‡æœ¬ä¸­ç§»é™¤çš„å­—å…ƒã€‚

    Returns:
        list: åŒ…å«å¾…æ¯”å°å¥å­çš„åˆ—è¡¨ã€‚
    """
    print("ğŸ”„ï¸ æ­£åœ¨è¼‰å…¥æ¯”å°æ–‡æœ¬çš„å¥å­...")  # å°å‡ºè¼‰å…¥è³‡æ–™çš„è¨Šæ¯ã€‚
    pattern = "[" + re.escape(chars_to_remove) + "]"  # ä½¿ç”¨æ­£è¦è¡¨é”å¼å»ºç«‹ä¸€å€‹å­—å…ƒæ¨¡å¼ï¼Œç”¨æ–¼åŒ¹é…éœ€è¦ç§»é™¤çš„å­—å…ƒã€‚
    all_sents = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜å¾…æ¯”å°çš„å¥å­ã€‚
    for fp in folder_path.rglob("*.txt"):  # éè¿´åœ°éæ­·è³‡æ–™å¤¾ä¸­æ‰€æœ‰ä»¥ .txt çµå°¾çš„æª”æ¡ˆã€‚
        text = fp.read_text(encoding="utf-8")  # ä»¥ UTF-8 ç·¨ç¢¼è®€å–æª”æ¡ˆçš„å…§å®¹ã€‚
        raw = re.split(pattern, text)  # ä½¿ç”¨æ­£è¦è¡¨é”å¼å°‡æ–‡æœ¬åˆ†å‰²æˆå¥å­ï¼Œä¸¦ç§»é™¤æŒ‡å®šçš„å­—å…ƒã€‚
        cleaned = [clean_sentence(s) for s in raw if isinstance(s, str) and s.strip()]  # æ¸…ç†æ¯å€‹å¥å­ï¼Œä¸¦æ’é™¤ç©ºå­—ä¸²ã€‚
        all_sents.extend(cleaned)  # å°‡æ¸…ç†å¾Œçš„å¥å­æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(all_sents)} å¥å¾…æ¯”å°å¥å­ã€‚")  # å°å‡ºè³‡æ–™è¼‰å…¥å®Œæˆçš„è¨Šæ¯ï¼Œä¸¦é¡¯ç¤ºè¼‰å…¥çš„å¥å­æ•¸é‡ã€‚
    return all_sents  # è¿”å›åŒ…å«å¾…æ¯”å°å¥å­çš„åˆ—è¡¨ã€‚

# ========== åˆ†è©å‡½å¼ ==========
def segment_in_batches(sentences, segmenter, batch_size=100):
    """
    å°‡å¥å­åˆ†æˆæ‰¹æ¬¡é€²è¡Œæ–·è©è™•ç†ã€‚

    Args:
        sentences (list): åŒ…å«éœ€è¦æ–·è©çš„å¥å­çš„åˆ—è¡¨ã€‚
        segmenter (CkipWordSegmenter): CKIP æ–·è©å™¨ç‰©ä»¶ã€‚
        batch_size (int): æ¯å€‹æ‰¹æ¬¡åŒ…å«çš„å¥å­æ•¸é‡ã€‚

    Returns:
        list: åŒ…å«æ–·è©çµæœçš„åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ éƒ½æ˜¯ä¸€å€‹è©èªåˆ—è¡¨ã€‚
    """
    all_tokens = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜æ‰€æœ‰å¥å­çš„æ–·è©çµæœã€‚
    for i in range(0, len(sentences), batch_size):  # è¿­ä»£è™•ç†æ‰€æœ‰å¥å­ï¼Œæ¯æ¬¡è™•ç†ä¸€å€‹æ‰¹æ¬¡ã€‚
        batch = sentences[i:i+batch_size]  # ç²å–ç•¶å‰æ‰¹æ¬¡çš„å¥å­ã€‚
        toks = segmenter(batch)  # ä½¿ç”¨ CKIP æ–·è©å™¨å°ç•¶å‰æ‰¹æ¬¡çš„å¥å­é€²è¡Œæ–·è©ã€‚
        all_tokens.extend(toks)  # å°‡ç•¶å‰æ‰¹æ¬¡çš„æ–·è©çµæœæ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
        del toks, batch  # åˆªé™¤ä¸å†éœ€è¦çš„è®Šæ•¸ï¼Œé‡‹æ”¾è¨˜æ†¶é«”ã€‚
        torch.cuda.empty_cache(); gc.collect()  # æ¸…ç©º GPU å¿«å–è¨˜æ†¶é«”ï¼Œä¸¦åŸ·è¡Œåƒåœ¾å›æ”¶ã€‚
    return all_tokens  # è¿”å›åŒ…å«æ‰€æœ‰å¥å­æ–·è©çµæœçš„åˆ—è¡¨ã€‚

# ========== è©å½™è¡¨èˆ‡å‘é‡åŒ– ==========
def build_vocab(all_tokens):
    """
    å»ºç«‹è©å½™è¡¨ï¼Œå°‡æ¯å€‹è©èªæ˜ å°„åˆ°ä¸€å€‹å”¯ä¸€çš„ç´¢å¼•ã€‚

    Args:
        all_tokens (list): åŒ…å«æ‰€æœ‰å¥å­æ–·è©çµæœçš„åˆ—è¡¨ã€‚

    Returns:
        dict: è©å½™è¡¨å­—å…¸ï¼Œkey ç‚ºè©èªï¼Œvalue ç‚ºç´¢å¼•ã€‚
    """
    vocab = sorted({w for toks in all_tokens for w in toks})  # å¾æ‰€æœ‰æ–·è©çµæœä¸­æå–å”¯ä¸€çš„è©èªï¼Œä¸¦é€²è¡Œæ’åºã€‚
    return {w: idx for idx, w in enumerate(vocab)}  # å»ºç«‹è©å½™è¡¨å­—å…¸ï¼Œå°‡æ¯å€‹è©èªæ˜ å°„åˆ°ä¸€å€‹å”¯ä¸€çš„ç´¢å¼•ã€‚

def vectorize_tokens(tokens_list, word2idx, device):
    """
    å°‡æ–·è©çµæœåˆ—è¡¨è½‰æ›æˆå‘é‡è¡¨ç¤ºã€‚

    Args:
        tokens_list (list): åŒ…å«æ‰€æœ‰å¥å­æ–·è©çµæœçš„åˆ—è¡¨ã€‚
        word2idx (dict): è©å½™è¡¨å­—å…¸ã€‚
        device (torch.device): ç”¨æ–¼å„²å­˜å‘é‡çš„è£ç½®ï¼ˆCPU æˆ– GPUï¼‰ã€‚

    Returns:
        torch.Tensor: å½¢ç‹€ç‚º (n_sentences, vocab_size) çš„å¼µé‡ï¼ŒåŒ…å«æ‰€æœ‰å¥å­çš„å‘é‡è¡¨ç¤ºã€‚
    """
    # å¯èƒ½æœƒç”¢ç”Ÿ (n_sentences Ã— vocab_size) çš„å¤§å¼µé‡ï¼Œæ³¨æ„è¨˜æ†¶é«”
    vec = torch.zeros((len(tokens_list), len(word2idx)), device=device)  # åˆå§‹åŒ–ä¸€å€‹å…¨é›¶å¼µé‡ï¼Œç”¨æ–¼å„²å­˜æ‰€æœ‰å¥å­çš„å‘é‡è¡¨ç¤ºã€‚
    for i, toks in enumerate(tokens_list):  # è¿­ä»£è™•ç†æ¯å€‹å¥å­çš„æ–·è©çµæœã€‚
        for w in toks:  # è¿­ä»£è™•ç†å¥å­ä¸­çš„æ¯å€‹è©èªã€‚
            idx = word2idx.get(w)  # å¾è©å½™è¡¨ä¸­ç²å–è©èªçš„ç´¢å¼•ã€‚
            if idx is not None:  # å¦‚æœè©èªåœ¨è©å½™è¡¨ä¸­ã€‚
                vec[i, idx] = 1  # å°‡å‘é‡ä¸­å°æ‡‰ç´¢å¼•çš„å€¼è¨­ç‚º 1ï¼Œè¡¨ç¤ºè©²è©èªåœ¨å¥å­ä¸­å‡ºç¾ã€‚
    return vec  # è¿”å›åŒ…å«æ‰€æœ‰å¥å­å‘é‡è¡¨ç¤ºçš„å¼µé‡ã€‚

# ========== Jaccard è¨ˆç®— ==========
def batch_jaccard(compared_vecs, origin_vecs):
    """
    è¨ˆç®—å…©å€‹å‘é‡æ‰¹æ¬¡ä¹‹é–“çš„ Jaccard ç›¸ä¼¼åº¦ã€‚

    Args:
        compared_vecs (torch.Tensor): å½¢ç‹€ç‚º (batch_size, vocab_size) çš„å¼µé‡ï¼ŒåŒ…å«å¾…æ¯”å°å¥å­çš„å‘é‡è¡¨ç¤ºã€‚
        origin_vecs (torch.Tensor): å½¢ç‹€ç‚º (num_origin_sentences, vocab_size) çš„å¼µé‡ï¼ŒåŒ…å«åŸå§‹å¥å­çš„å‘é‡è¡¨ç¤ºã€‚

    Returns:
        torch.Tensor: å½¢ç‹€ç‚º (batch_size, num_origin_sentences) çš„å¼µé‡ï¼ŒåŒ…å«æ¯å°å¥å­ä¹‹é–“çš„ Jaccard ç›¸ä¼¼åº¦ã€‚
    """
    inter = torch.matmul(compared_vecs, origin_vecs.T)  # è¨ˆç®—äº¤é›†å¤§å°ï¼Œä½¿ç”¨çŸ©é™£ä¹˜æ³•ã€‚
    sum_c = compared_vecs.sum(1, keepdim=True)  # è¨ˆç®—å¾…æ¯”å°å¥å­ä¸­éé›¶å…ƒç´ çš„å’Œï¼Œkeepdim=True ä¿æŒç¶­åº¦ï¼Œä»¥ä¾¿å¾ŒçºŒè¨ˆç®—ã€‚
    sum_o = origin_vecs.sum(1, keepdim=True).T  # è¨ˆç®—åŸå§‹å¥å­ä¸­éé›¶å…ƒç´ çš„å’Œï¼Œä¸¦è½‰ç½®ï¼Œä»¥ä¾¿å¾ŒçºŒè¨ˆç®—ã€‚
    union = sum_c + sum_o - inter + 1e-9  # è¨ˆç®—è¯é›†å¤§å°ï¼ŒåŠ ä¸Šä¸€å€‹æ¥µå°å€¼ä»¥é¿å…é™¤ä»¥é›¶çš„éŒ¯èª¤ã€‚
    return inter / union  # è¿”å› Jaccard ç›¸ä¼¼åº¦ï¼Œå³äº¤é›†å¤§å°é™¤ä»¥è¯é›†å¤§å°ã€‚

# ========== ä¸»ç¨‹å¼ ==========
def main():
    """
    ç¨‹å¼çš„ä¸»å‡½æ•¸ï¼Œè² è²¬å”èª¿å„å€‹éƒ¨åˆ†ï¼Œå®Œæˆæ–‡æœ¬æ¯”å°çš„ä»»å‹™ã€‚
    """
    origin_sents = load_parsed_results(PARSED_RESULTS_PATH)  # è¼‰å…¥åŸå§‹å¥å­ã€‚
    compared_sents = load_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)  # è¼‰å…¥å¾…æ¯”å°çš„å¥å­ã€‚

    print("ğŸªš åˆ†è©è™•ç†...")  # å°å‡ºåˆ†è©è™•ç†é–‹å§‹çš„è¨Šæ¯ã€‚
    ws = CkipWordSegmenter(device=CKIP_DEVICE, model="bert-base")  # åˆå§‹åŒ– CKIP æ–·è©å™¨ã€‚
    origin_tokens = segment_in_batches(origin_sents, ws, batch_size=100)  # å°åŸå§‹å¥å­é€²è¡Œåˆ†è©ã€‚
    compared_tokens = segment_in_batches(compared_sents, ws, batch_size=100)  # å°å¾…æ¯”å°å¥å­é€²è¡Œåˆ†è©ã€‚
    del ws; torch.cuda.empty_cache(); gc.collect()  # åˆªé™¤æ–·è©å™¨ç‰©ä»¶ï¼Œæ¸…ç©º GPU å¿«å–è¨˜æ†¶é«”ï¼Œä¸¦åŸ·è¡Œåƒåœ¾å›æ”¶ã€‚

    print("â¡ï¸ å»ºæ§‹è©å½™...")  # å°å‡ºå»ºæ§‹è©å½™è¡¨é–‹å§‹çš„è¨Šæ¯ã€‚
    word2idx = build_vocab(origin_tokens)  # å»ºç«‹è©å½™è¡¨ã€‚

    # **é‡è¦ï¼šä»¥ä¸‹å‘é‡åŒ–origin_vecsåœ¨CPUä¸ŠåŸ·è¡Œï¼Œä»¥é¿å…ä¸€æ¬¡æ€§å¤§å¼µé‡åœ¨GPUä¸ŠOOM**
    print("   - originå‘é‡åŒ– (CPU)...")
    origin_vecs_cpu = vectorize_tokens(origin_tokens, word2idx, device=torch.device("cpu"))  # åœ¨ CPU ä¸Šå°‡åŸå§‹å¥å­è½‰æ›æˆå‘é‡è¡¨ç¤ºã€‚
    print(f"     origin_vecs CPU å¤§å°: {origin_vecs_cpu.numel()*4/1024**3:.2f} GiB (byte) ")  # å°å‡ºåŸå§‹å¥å­å‘é‡è¡¨ç¤ºçš„å¤§å°ã€‚
    # å°‡è¼ƒå°çš„origin_vecsæ¬åˆ°GPU
    origin_vecs = origin_vecs_cpu.to(DEVICE)  # å°‡åŸå§‹å¥å­çš„å‘é‡è¡¨ç¤ºè¤‡è£½åˆ° GPU ä¸Šã€‚
    del origin_vecs_cpu, origin_tokens  # åˆªé™¤ CPU ä¸Šçš„å‘é‡è¡¨ç¤ºå’ŒåŸå§‹æ–·è©çµæœï¼Œé‡‹æ”¾è¨˜æ†¶é«”ã€‚
    torch.cuda.empty_cache(); gc.collect()  # æ¸…ç©º GPU å¿«å–è¨˜æ†¶é«”ï¼Œä¸¦åŸ·è¡Œåƒåœ¾å›æ”¶ã€‚

    print("ğŸ§ª Batch Jaccard & åŒ¹é…...")  # å°å‡º Jaccard ç›¸ä¼¼åº¦è¨ˆç®—å’ŒåŒ¹é…é–‹å§‹çš„è¨Šæ¯ã€‚
    matches = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜åŒ¹é…çµæœã€‚
    bs = 256  # è¨­å®šæ‰¹æ¬¡å¤§å°ã€‚
    total_batches = (len(compared_tokens)+bs-1)//bs  # è¨ˆç®—ç¸½æ‰¹æ¬¡æ•¸ã€‚
    pbar = tqdm(total=total_batches, desc="ç¸½é€²åº¦")  # åˆå§‹åŒ–é€²åº¦æ¢ã€‚

    # ä½¿ç”¨ DataLoader è™•ç† compared_tokens
    compared_dataset = TensorDataset(torch.arange(len(compared_tokens)))  # å‰µå»ºä¸€å€‹åŒ…å« compared_tokens ç´¢å¼•çš„ TensorDatasetã€‚
    compared_loader = DataLoader(compared_dataset, batch_size=bs, shuffle=False)  # ä½¿ç”¨ DataLoader è¼‰å…¥ compared_tokens çš„ç´¢å¼•ã€‚

    for batch_indices in compared_loader:  # è¿­ä»£è™•ç†æ¯å€‹æ‰¹æ¬¡çš„ç´¢å¼•ã€‚
        batch_start = batch_indices[0][0].item()  # ç²å–ç•¶å‰æ‰¹æ¬¡çš„èµ·å§‹ç´¢å¼•ã€‚
        batch_end = min(batch_start + bs, len(compared_tokens))  # è¨ˆç®—ç•¶å‰æ‰¹æ¬¡çš„çµæŸç´¢å¼•ã€‚
        batch_tokens = compared_tokens[batch_start:batch_end]  # ç²å–ç•¶å‰æ‰¹æ¬¡çš„æ–·è©çµæœã€‚

        # **æ³¨æ„ï¼šé€™è£¡ comp_vecs å¤§å°ä¹Ÿå— bs èˆ‡ vocab_size å½±éŸ¿ï¼Œç¢ºä¿ bs ä¸è¶…éå¯ç”¨ GPU è¨˜æ†¶é«”**
        comp_vecs = vectorize_tokens(batch_tokens, word2idx, device=DEVICE)  # å°‡ç•¶å‰æ‰¹æ¬¡çš„æ–·è©çµæœè½‰æ›æˆå‘é‡è¡¨ç¤ºã€‚
        try:
            jacc = batch_jaccard(comp_vecs, origin_vecs)  # è¨ˆç®—ç•¶å‰æ‰¹æ¬¡èˆ‡åŸå§‹å¥å­ä¹‹é–“çš„ Jaccard ç›¸ä¼¼åº¦ã€‚
        except torch.cuda.OutOfMemoryError:  # å¦‚æœç™¼ç”Ÿ GPU è¨˜æ†¶é«”ä¸è¶³çš„éŒ¯èª¤ã€‚
            torch.cuda.empty_cache(); gc.collect()  # æ¸…ç©º GPU å¿«å–è¨˜æ†¶é«”ï¼Œä¸¦åŸ·è¡Œåƒåœ¾å›æ”¶ã€‚
            bs = max(bs//2, 1)  # å°‡æ‰¹æ¬¡å¤§å°æ¸›åŠï¼Œæœ€å°ç‚º 1ã€‚
            print(f"âš ï¸ OOMï¼Œé™åˆ° bs={bs}")  # å°å‡ºè¨˜æ†¶é«”ä¸è¶³çš„è¨Šæ¯ï¼Œä¸¦é¡¯ç¤ºæ–°çš„æ‰¹æ¬¡å¤§å°ã€‚
            compared_loader = DataLoader(compared_dataset, batch_size=bs, shuffle=False)  # æ›´æ–° DataLoader çš„æ‰¹æ¬¡å¤§å°ã€‚
            pbar.total = (len(compared_tokens) + bs - 1) // bs;  # æ›´æ–°é€²åº¦æ¢çš„ç¸½æ‰¹æ¬¡æ•¸ã€‚
            pbar.refresh()  # åˆ·æ–°é€²åº¦æ¢ã€‚
            continue  # è·³åˆ°ä¸‹ä¸€å€‹æ‰¹æ¬¡çš„è™•ç†ã€‚
        scores, idxs = jacc.max(1)  # ç²å–æ¯å€‹å¾…æ¯”å°å¥å­èˆ‡åŸå§‹å¥å­ä¹‹é–“çš„æœ€å¤§ Jaccard ç›¸ä¼¼åº¦ï¼Œä»¥åŠå°æ‡‰çš„åŸå§‹å¥å­ç´¢å¼•ã€‚
        for idx_in_batch, (s, scr, idx_o) in enumerate(zip(batch_tokens, scores.tolist(), idxs.tolist())):  # è¿­ä»£è™•ç†æ‰¹æ¬¡ä¸­çš„æ¯å€‹å¥å­åŠå…¶åŒ¹é…çµæœã€‚
            if scr >= JACCARD_THRESHOLD:  # å¦‚æœç›¸ä¼¼åº¦è¶…éè¨­å®šçš„é–¾å€¼ã€‚
                matches.append({  # å°‡åŒ¹é…çµæœä»¥å­—å…¸å½¢å¼å„²å­˜ã€‚
                    "Comparedå¥å­": compared_sents[batch_start + idx_in_batch],  # ä½¿ç”¨åŸå§‹ç´¢å¼•ç²å– compared_sents
                    "å°æ‡‰åŸå¥": origin_sents[idx_o],  # å„²å­˜å°æ‡‰çš„åŸå§‹å¥å­ã€‚
                    "Jaccardç›¸ä¼¼åº¦": scr  # å„²å­˜ Jaccard ç›¸ä¼¼åº¦åˆ†æ•¸ã€‚
                })
        del comp_vecs, jacc  # åˆªé™¤ä¸å†éœ€è¦çš„è®Šæ•¸ï¼Œé‡‹æ”¾è¨˜æ†¶é«”ã€‚
        torch.cuda.empty_cache(); gc.collect()  # æ¸…ç©º GPU å¿«å–è¨˜æ†¶é«”ï¼Œä¸¦åŸ·è¡Œåƒåœ¾å›æ”¶ã€‚
        pbar.update(1)  # æ›´æ–°é€²åº¦æ¢ã€‚
    pbar.close()  # é—œé–‰é€²åº¦æ¢ã€‚

    print(f"âœ… å®Œæˆï¼Œå…± {len(matches)} ç­†çµæœï¼Œä¿å­˜ä¸­...")  # å°å‡ºåŒ¹é…å®Œæˆå’Œé–‹å§‹å„²å­˜çš„è¨Šæ¯ã€‚
    json.dump(matches, OUTPUT_JSON_PATH.open('w', encoding='utf-8'), ensure_ascii=False, indent=2)  # å°‡åŒ¹é…çµæœä»¥ JSON æ ¼å¼å¯«å…¥æª”æ¡ˆï¼Œensure_ascii=False ç¢ºä¿ä¸­æ–‡ä¸è¢«è½‰ç¾©ï¼Œindent=2 ä½¿è¼¸å‡ºæ›´æ˜“è®€ã€‚
    print(f"ğŸ“„ è¼¸å‡º: {OUTPUT_JSON_PATH}")  # å°å‡ºè¼¸å‡ºæª”æ¡ˆçš„è·¯å¾‘ã€‚

if __name__ == '__main__':
    main()  # åŸ·è¡Œä¸»ç¨‹å¼ã€‚
