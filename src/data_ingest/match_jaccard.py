import json
import re
from pathlib import Path
from tqdm import tqdm
import torch
import gc
from torch.utils.data import DataLoader, TensorDataset
import time
from ckip_transformers.nlp import CkipWordSegmenter
import cupy as cp  # GPU acceleration with CuPy

# ========== ä½¿ç”¨è€…è¨­å®š (ä¸€è™•å¯æ§) ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json") # è¨­å®šç‚ºåŸå¥è³‡æ–™è¼¸å…¥è·¯å¾‘
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/compared_text/è«¸å­") # è¨­å®šç‚ºæ¯”å°è³‡æ–™å¤¾è¼¸å…¥è·¯å¾‘
OUTPUT_JSON_PATH = Path(r"D:/lufu_allusion/data/processed/ZI_match_results_jaccard.json") # è¨­å®šç‚ºæ¯”å°çµæœè¼¸å‡ºè·¯å¾‘
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\#\\-\\ï¼\\(\\)\\[\\]\\]\\/(),1234567890Â¶" # æ¯”å°è³‡æ–™æ¸…æ´—å­—å…ƒ
JACCARD_THRESHOLD = 0.7       # ç›¸ä¼¼åº¦é–¾å€¼ï¼Œå¯èª¿æ•´
BATCH_SIZE = 4096             # ç”¨æˆ¶å¯èª¿æ•´çš„æ‰¹æ¬¡å¤§å°
MIN_BATCH_SIZE = 512          # æœ€å°æ‰¹æ¬¡å¤§å°
# ORIGIN_CHUNK_SIZE will be set dynamically after tokenization
ORIGIN_CHUNK_SIZE = None      # placeholder

# ========== è¨­å‚™æª¢æ¸¬ ==========
USE_GPU = True
if USE_GPU and cp.cuda.runtime.getDeviceCount() > 0:
    DEVICE = 'cuda'
    print(f"âœ… ä½¿ç”¨ GPU åŠ é€Ÿ (CuPy Devices={cp.cuda.runtime.getDeviceCount()})")
else:
    DEVICE = 'cpu'
    USE_GPU = False
    print("âš ï¸ æœªåµæ¸¬åˆ°å¯ç”¨ GPUï¼Œé€€å› CPU é‹ç®—ã€‚")

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [ # åŸå¥è‹¥ä»¥ä¸‹åˆ—è©å½™èµ·å§‹å‰‡åˆªé™¤è©²è©å½™
    "å¾’è§€å…¶", "çŸå¤«", "çŸä¹ƒ", "è‡³å¤«", "æ‡¿å¤«", "è“‹ç”±æˆ‘å›", "é‡æ›°", "æ˜¯çŸ¥", "å—Ÿå¤«", "å¤«å…¶", "æ‡¿å…¶", "æ‰€ä»¥",
    "æƒ³å¤«", "å…¶å§‹ä¹Ÿ", "ç•¶å…¶", "æ³å¾©", "æ™‚å‰‡", "è‡³è‹¥", "è±ˆç¨", "è‹¥ä¹ƒ", "ä»Šå‰‡", "ä¹ƒçŸ¥", "æ—¢è€Œ", "å—Ÿä¹",
    "æ•…æˆ‘å", "è§€å¤«", "ç„¶è€Œ", "çˆ¾ä¹ƒ", "æ˜¯ä»¥", "åŸå¤«", "æ›·è‹¥", "æ–¯å‰‡", "æ–¼æ™‚", "æ–¹ä»Š", "äº¦ä½•å¿…", "è‹¥ç„¶",
    "å®¢æœ‰", "è‡³æ–¼", "å‰‡çŸ¥", "ä¸”å¤«", "æ–¯ä¹ƒ", "æ³", "æ–¼æ˜¯", "è¦©å¤«", "ä¸”å½¼", "è±ˆè‹¥", "å·²è€Œ", "å§‹ä¹Ÿ", "æ•…",
    "ç„¶å‰‡", "è±ˆå¦‚æˆ‘", "è±ˆä¸ä»¥", "æˆ‘åœ‹å®¶", "å…¶å·¥è€…", "æ‰€è¬‚", "ä»Šå¾å›", "åŠå¤«", "çˆ¾å…¶", "å°‡ä»¥", "å¯ä»¥",
    "ä»Š", "åœ‹å®¶", "ç„¶å¾Œ", "å‘éæˆ‘å", "å‰‡æœ‰", "å½¼", "æƒœä¹", "ç”±æ˜¯", "ä¹ƒè¨€æ›°", "è‹¥å¤«", "äº¦ä½•ç”¨", "ä¸ç„¶",
    "å˜‰å…¶", "ä»Šå‰‡", "å¾’ç¾å¤«", "æ•…èƒ½", "æœ‰æ¢è€…æ›°", "æƒœå¦‚", "è€Œæ³", "é€®å¤«", "èª å¤«", "æ–¼æˆ²", "æ´ä¹", "ä¼Šæ˜”",
    "å‰‡å°‡", "ä»Šå‰‡", "æ³ä»Š", "å£«æœ‰", "æš¨ä¹", "äº¦ä½•è¾¨å¤«", "ä¿¾å¤«", "äº¦çŒ¶", "ç»å¤«", "æ™‚ä¹Ÿ", "å›ºçŸ¥", "è¶³ä»¥",
    "çŸåœ‹å®¶", "æ¯”ä¹", "äº¦ç”±", "è§€å…¶", "å°‡ä¿¾ä¹", "è–äºº", "å›å­", "æ–¼ä»¥", "ä¹ƒ", "æ–¯è“‹", "å™«", "å¤«æƒŸ",
    "é«˜çš‡å¸", "å¸æ—¢", "å˜‰å…¶", "å§‹å‰‡", "åˆå®‰å¾—", "å…¶", "å„’æœ‰", "ç•¶æ˜¯æ™‚ä¹Ÿ", "å¤«ç„¶", "å®œä¹", "æ•…å…¶", "åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ", "ä»Šæˆ‘åœ‹å®¶", "æ˜¯æ™‚", "æœ‰å¸", "å‘è‹¥", "æˆ‘çš‡", "æ•…ç‹è€…", "å‰‡", "é„’å­", "å­°", "æš¨å¤«", "ç”¨èƒ½",
    "æ•…å°‡", "æ³å…¶", "æ•…å®œ", "ç‹è€…", "è–ä¸Š", "å…ˆç‹", "ä¹ƒæœ‰", "æ³ä¹ƒ", "åˆ¥æœ‰", "ä»Šè€…", "å›ºå®œ", "çš‡ä¸Š", "ä¸”å…¶",
    "å¾’è§€å¤«", "å¸å ¯ä»¥", "å§‹å…¶", "å€è€Œ", "ä¹ƒæ›°", "å‘ä½¿", "æ¼¢æ­¦å¸", "å…ˆæ˜¯", "ä»–æ—¥", "ä¹ƒå‘½", "è§€ä¹", "åœ‹å®¶ä»¥",
    "å¢¨å­", "å€Ÿå¦‚", "è¶³ä»¥", "ä¸Šä¹ƒ", "å—šå‘¼", "æ˜”ä¼Š", "å…ˆè³¢", "é‚ä½¿", "è±ˆæ¯”å¤«", "å›ºå…¶", "æ³æœ‰", "é­¯æ­ç‹", "çš‡å®¶",
    "å¾å›æ˜¯æ™‚", "çŸ¥", "å‘¨ç©†ç‹", "å‰‡æœ‰", "æ˜¯ç”¨", "ä¹ƒè¨€æ›°", "åŠ", "æ•…å¤«", "çŸä¹", "å¤«ä»¥", "å¯§ä»¤", "å¦‚", "ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ", "é‚", "æ‚²å¤«", "å®‰å¾—", "æ•…å¾—", "ä¸”è¦‹å…¶", "æ˜¯ä½•", "è«ä¸", "å£«æœ‰", "çŸ¥å…¶", "æœªè‹¥"
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰", "ä¹", "ç„‰"] # åŸå¥è‹¥ä»¥ä¸‹åˆ—è©å½™çµå°¾å‰‡åˆªé™¤è©²è©å½™

def clean_sentence(text):
    for prefix in PREFIX_EXCLUDE:
        if text.startswith(prefix):
            return text[len(prefix):].strip()
    for suffix in SUFFIX_EXCLUDE:
        if text.endswith(suffix):
            return text[:-len(suffix)].strip()
    return text.strip()

# ========== è®€å–èˆ‡æ¸…æ´—è³‡æ–™ ==========
def load_parsed_results(json_path):
    print("ğŸ”„ï¸ è¼‰å…¥åŸå¥è³‡æ–™...")
    with open(json_path, encoding="utf-8") as f:
        data = json.load(f)
    records = []
    for article in data:
        for para in article.get("æ®µè½", []):
            for group in para.get("å¥çµ„", []):
                for sent in group.get("å¥å­", []):
                    clean = clean_sentence(sent.get("å…§å®¹", ""))
                    if clean:
                        records.append({
                            "article_num": article.get("ç¯‡è™Ÿ"),
                            "author": article.get("è³¦å®¶", ""),
                            "article_title": article.get("è³¦ç¯‡", ""),
                            "paragraph_num": para.get("æ®µè½ç·¨è™Ÿ"),
                            "group_num": group.get("å¥çµ„ç·¨è™Ÿ"),
                            "sentence_num": sent.get("å¥ç·¨è™Ÿ"),
                            "original": sent.get("å…§å®¹", ""),
                            "cleaned": clean
                        })
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} æ¢åŸå¥ã€‚")
    return records


def load_compared_sentences(folder_path, chars_to_remove):
    print("ğŸ”„ï¸ è¼‰å…¥æ¯”å°å¥è³‡æ–™...")
    pattern = "[" + re.escape(chars_to_remove) + "]"
    sents = []
    for fp in folder_path.rglob("*.txt"):
        raw = fp.read_text(encoding="utf-8").replace("\n", "")
        raw = re.sub(r"<[^>]*>", "", raw)  # åˆªé™¤æ‰€æœ‰ <...> æ¨™è¨˜
        for idx, seg in enumerate(re.split(pattern, raw)):
            clean = clean_sentence(seg)
            if clean:
                sents.append({
                    "matched_file": fp.parent.name + "/" + fp.stem,
                    "matched_index": idx,
                    "raw": seg,
                    "cleaned": clean
                })
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(sents)} æ¢å¾…æ¯”å°å¥ã€‚")
    return sents

# ========== åˆ†è© ==========
def segment_in_batches(sentences, segmenter, batch_size=100, text_type=""):
    print(f"ğŸªš åˆ†è©è™•ç†ï¼š{text_type}ï¼ˆå…± {len(sentences)} æ¢ï¼‰...")
    all_tokens = []
    with tqdm(total=len(sentences), desc=f"åˆ†è© ({text_type})") as pbar:
        for i in range(0, len(sentences), batch_size):
            batch = [s["cleaned"] for s in sentences[i:i+batch_size]]
            toks = segmenter(batch, show_progress=False)
            all_tokens.extend(toks)
            pbar.update(len(batch))
            del toks, batch
            torch.cuda.empty_cache(); gc.collect()
    print(f"âœ… {text_type} åˆ†è©å®Œæˆã€‚")
    return all_tokens

# ========== è©å½™è¡¨å»ºç«‹ ==========
def build_vocab(tokens_list):
    print("â¡ï¸ å»ºæ§‹è©å½™è¡¨...")
    vocab = sorted({w for toks in tokens_list for w in toks})
    print(f"âœ… è©å½™è¡¨å¤§å°ï¼š{len(vocab)} å€‹è©ã€‚")
    return {w: i for i, w in enumerate(vocab)}

# ========== å‘é‡åŒ–åˆ° GPU åˆ†å¡Š ==========
def tokens_to_gpu_matrix(tokens_list, word2idx, chunk_size):
    mats = []
    for st in range(0, len(tokens_list), chunk_size):
        ed = min(st + chunk_size, len(tokens_list))
        mat = cp.zeros((ed - st, len(word2idx)), dtype=cp.int8)
        for i, toks in enumerate(tokens_list[st:ed]):
            for w in toks:
                idx = word2idx.get(w)
                if idx is not None:
                    mat[i, idx] = 1
        mats.append(mat)
    return mats

# ========== æ‰¹æ¬¡ Jaccard è¨ˆç®— (CuPy) ==========
def batch_jaccard_gpu(comp_mat, origin_mats):
    best_s = cp.zeros(comp_mat.shape[0], dtype=cp.float32)
    best_i = cp.zeros(comp_mat.shape[0], dtype=cp.int32)
    for idx, om in enumerate(origin_mats):
        inter = comp_mat.dot(om.T)
        sc = comp_mat.sum(axis=1, keepdims=True)
        so = om.sum(axis=1, keepdims=True).T
        jac = inter / (sc + so - inter + 1e-9)
        mv = jac.max(axis=1)
        mi = jac.argmax(axis=1) + idx * om.shape[0]
        mask = mv > best_s
        best_s[mask] = mv[mask]
        best_i[mask] = mi[mask]
        del inter, sc, so, jac
        cp.get_default_memory_pool().free_all_blocks()
    return best_s, best_i

# ========== ä¸»ç¨‹å¼ ==========
def main():
    origin_data = load_parsed_results(PARSED_RESULTS_PATH)
    compared_data = load_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)

    # åˆ†è©
    ws = CkipWordSegmenter(device=0 if USE_GPU else -1, model="bert-base")
    origin_tokens = segment_in_batches(origin_data, ws, batch_size=100, text_type="åŸå§‹æ–‡æœ¬")
    compared_tokens = segment_in_batches(compared_data, ws, batch_size=100, text_type="æ¯”å°æ–‡æœ¬")
    del ws; torch.cuda.empty_cache(); gc.collect()

    # å»ºè©è¡¨
    word2idx = build_vocab(origin_tokens)

        # å‹•æ…‹è¨­å®šåˆ†å¡Šå¤§å°
    ORIGIN_CHUNK_SIZE = len(origin_tokens)

    # å‘é‡åŒ– origin
    if USE_GPU:
        print(f"ğŸ”¢ å°‡åŸå¥ tokens å‘é‡åŒ–ä¸¦åˆ‡å¡Šä¸Š GPU (chunk_size={ORIGIN_CHUNK_SIZE})...")
        origin_mats = tokens_to_gpu_matrix(origin_tokens, word2idx, ORIGIN_CHUNK_SIZE)
    else:
        origin_vecs_cpu = vectorize_tokens(origin_tokens, word2idx, device=torch.device('cpu'))

    # Jaccard & åŒ¹é…
    matches = []
    total = len(compared_data)
    print("ğŸ§ª Jaccard & åŒ¹é…... ")
    for st in tqdm(range(0, total, BATCH_SIZE), desc="Jaccard & åŒ¹é…"):
        ed = min(st + BATCH_SIZE, total)
        batch_t = compared_tokens[st:ed]
        if USE_GPU:
            comp_mat = tokens_to_gpu_matrix(batch_t, word2idx, BATCH_SIZE)[0]
            sc, ix = batch_jaccard_gpu(comp_mat, origin_mats)
            sc, ix = cp.asnumpy(sc), cp.asnumpy(ix)
            cp.get_default_memory_pool().free_all_blocks()
        else:
            comp_vecs = vectorize_tokens(batch_t, word2idx, device=torch.device('cpu'))
            jacc = batch_jaccard_cpu(comp_vecs, origin_vecs_cpu)
            sc, ix = jacc.max(1).numpy(), jacc.argmax(1).numpy()
            del comp_vecs, jacc

        for i, (s, oid) in enumerate(zip(sc, ix)):
            if s >= JACCARD_THRESHOLD:
                od = origin_data[oid]; cd = compared_data[st + i]
                matches.append({
                    **{k: od[k] for k in ["article_num","author","article_title","paragraph_num","group_num","sentence_num","original"]},
                    **{"matched_file": cd["matched_file"],"matched_index": cd["matched_index"],"matched": cd["raw"],"similarity": float(s)}
                })

    print(f"âœ… åŒ¹é…å®Œæˆï¼Œå…± {len(matches)} ç­†çµæœã€‚")
    print("ğŸ“„ æ’åºä¸¦è¼¸å‡º JSON...")
    matches = sorted(matches, key=lambda x: (
        int(x["article_num"]), int(x["paragraph_num"]), int(x["group_num"]), int(x["sentence_num"])  
    ))
    with OUTPUT_JSON_PATH.open("w", encoding="utf-8") as f:
        json.dump(matches, f, ensure_ascii=False, indent=2)
    print(f"âœ… å®Œæˆï¼è¼¸å‡ºæª”æ¡ˆï¼š{OUTPUT_JSON_PATH}")

if __name__ == "__main__":
    main()
