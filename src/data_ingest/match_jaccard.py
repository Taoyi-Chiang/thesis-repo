import json  # å°å…¥ json æ¨¡çµ„ï¼Œç”¨æ–¼è™•ç† JSON æ ¼å¼çš„è³‡æ–™ã€‚
import pandas as pd  # å°å…¥ pandas æ¨¡çµ„ï¼Œé›–ç„¶åœ¨é€™å€‹ç¨‹å¼ä¸­æœªä½¿ç”¨ï¼Œä½†é€šå¸¸ç”¨æ–¼è³‡æ–™åˆ†æå’Œè™•ç†ã€‚
import re  # å°å…¥ re æ¨¡çµ„ï¼Œç”¨æ–¼è™•ç†æ­£è¦è¡¨é”å¼ï¼Œé€²è¡Œå­—ä¸²åŒ¹é…å’Œè™•ç†ã€‚
from pathlib import Path  # å°å…¥ pathlib æ¨¡çµ„ä¸­çš„ Path é¡åˆ¥ï¼Œç”¨æ–¼è™•ç†æª”æ¡ˆè·¯å¾‘ï¼Œæä¾›æ›´ç›´è§€çš„æ“ä½œæ–¹å¼ã€‚
from tqdm import tqdm  # å°å…¥ tqdm æ¨¡çµ„ï¼Œç”¨æ–¼é¡¯ç¤ºè¿´åœˆçš„é€²åº¦æ¢ï¼Œæä¾›ä½¿ç”¨è€…å‹å–„çš„ä»‹é¢ã€‚
import torch  # å°å…¥ torch æ¨¡çµ„ï¼ŒPyTorch çš„ä¸»è¦æ¨¡çµ„ï¼Œç”¨æ–¼æ·±åº¦å­¸ç¿’å’Œ GPU åŠ é€Ÿã€‚
from ckip_transformers.nlp import CkipWordSegmenter  # å°å…¥ CkipWordSegmenter é¡åˆ¥ï¼Œç”¨æ–¼ä¸­æ–‡æ–·è©ã€‚
import time  # å°å…¥ time æ¨¡çµ„ï¼Œç”¨æ–¼è¨ˆç®—ç¨‹å¼åŸ·è¡Œæ™‚é–“ã€‚

# ========== ä½¿ç”¨è€…è¨­å®š ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")  # å®šç¾©åŸå§‹è§£æçµæœ JSON æª”æ¡ˆçš„è·¯å¾‘ã€‚
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/compared_text")  # å®šç¾©åŒ…å«å¾…æ¯”å°æ–‡æœ¬æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ã€‚
OUTPUT_JSON_PATH = Path(r"D:/lufu_allusion/data/processed/sample_match_results_jaccard_gpu.json")  # å®šç¾©è¼¸å‡º JSON æª”æ¡ˆçš„è·¯å¾‘ï¼Œç”¨æ–¼å„²å­˜æ¯”å°çµæœã€‚
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\-\\ï¼\\(\\)\\[\\]/(),1234567890Â¶"  # å®šç¾©éœ€è¦å¾æ–‡æœ¬ä¸­ç§»é™¤çš„å­—å…ƒã€‚
JACCARD_THRESHOLD = 0.45  # å®šç¾© Jaccard ç›¸ä¼¼åº¦é–¾å€¼ï¼Œåªæœ‰é«˜æ–¼æ­¤å€¼çš„åŒ¹é…çµæœæ‰æœƒè¢«ä¿ç•™ã€‚

if torch.cuda.is_available():
    DEVICE = torch.device("cuda")  # å¦‚æœæœ‰ GPUï¼Œå‰‡å°‡è£ç½®è¨­å®šç‚ºä½¿ç”¨ GPUã€‚
    print(f"âœ… åµæ¸¬åˆ° GPU: {torch.cuda.get_device_name(0)}ï¼Œå°‡ä½¿ç”¨ GPU åŠ é€Ÿï¼")
    CKIP_DEVICE = 0  # è¨­å®š CKIP ä½¿ç”¨çš„ GPU è£ç½®ç·¨è™Ÿ
else:
    DEVICE = torch.device("cpu")  # å¦‚æœæ²’æœ‰ GPUï¼Œå‰‡ä½¿ç”¨ CPUã€‚
    print("âš ï¸ æ²’æœ‰åµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU é‹ç®—ã€‚")
    CKIP_DEVICE = -1 # è¨­å®š CKIP ä½¿ç”¨ CPU

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [
    "å¾’è§€å…¶", "çŸ§å¤«", "çŸ§ä¹ƒ", "è‡³å¤«", "æ‡¿å¤«", "è“‹ç”±æˆ‘å›", "é‡æ›°", "æ˜¯çŸ¥", "å¤«å…¶", "æ‡¿å…¶", "æ‰€ä»¥",
    "æƒ³å¤«", "å…¶å§‹ä¹Ÿ", "ç•¶å…¶", "æ³å¾©", "æ™‚å‰‡", "è‡³è‹¥", "è±ˆç¨", "è‹¥ä¹ƒ", "ä»Šå‰‡", "ä¹ƒçŸ¥", "æ—¢è€Œ", "å—Ÿä¹",
    "æ•…æˆ‘å", "è§€å¤«", "ç„¶è€Œ", "çˆ¾ä¹ƒ", "æ˜¯ä»¥", "åŸå¤«", "æ›·è‹¥", "æ–¯å‰‡", "æ–¼æ™‚", "æ–¹ä»Š", "äº¦ä½•å¿…", "è‹¥ç„¶",
    "å®¢æœ‰", "è‡³æ–¼", "å‰‡çŸ¥", "ä¸”å¤«", "æ–¯ä¹ƒ", "æ³", "æ–¼æ˜¯", "è¦©å¤«", "ä¸”å½¼", "è±ˆè‹¥", "å·²è€Œ", "å§‹ä¹Ÿ", "æ•…",
    "ç„¶å‰‡", "è±ˆå¦‚æˆ‘", "è±ˆä¸ä»¥", "æˆ‘åœ‹å®¶", "å…¶å·¥è€…", "æ‰€è¬‚", "ä»Šå¾å›", "åŠå¤«", "çˆ¾å…¶", "å°‡ä»¥", "å¯ä»¥",
    "ä»Š", "åœ‹å®¶", "ç„¶å¾Œ", "å‘éæˆ‘å", "å‰‡æœ‰", "å½¼", "æƒœä¹", "ç”±æ˜¯", "ä¹ƒè¨€æ›°", "è‹¥å¤«", "äº¦ä½•ç”¨", "ä¸ç„¶",
    "å˜‰å…¶", "ä»Šå‰‡", "å¾’ç¾å¤«", "æ•…èƒ½", "æœ‰æ¢è€…æ›°", "æƒœå¦‚", "è€Œæ³", "é€®å¤«", "èª å¤«", "æ–¼æˆ²", "æ´ä¹", "ä¼Šæ˜”",
    "å‰‡å°‡", "ä»Šå‰‡", "æ³ä»Š", "å£«æœ‰", "æš¨ä¹", "äº¦ä½•è¾¨å¤«", "ä¿¾å¤«", "äº¦çŒ¶", "ç»å¤«", "æ™‚ä¹Ÿ", "å›ºçŸ¥", "è¶³ä»¥",
    "çŸ§åœ‹å®¶", "æ¯”ä¹", "äº¦ç”±", "è§€å…¶", "å°‡ä¿¾ä¹", "è–äºº", "å›å­", "æ–¼ä»¥", "ä¹ƒ", "æ–¯è“‹", "å™«", "å¤«æƒŸ",
    "é«˜çš‡å¸", "å¸æ—¢", "å˜‰å…¶", "å§‹å‰‡", "åˆå®‰å¾—", "å…¶", "å„’æœ‰", "ç•¶æ˜¯æ™‚ä¹Ÿ", "å¤«ç„¶", "å®œä¹", "æ•…å…¶", "åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ", "ä»Šæˆ‘åœ‹å®¶", "æ˜¯æ™‚", "æœ‰å¸", "å‘è‹¥", "æˆ‘çš‡", "æ•…ç‹è€…", "å‰‡", "é„’å­", "å­°", "æš¨å¤«", "ç”¨èƒ½",
    "æ•…å°‡", "æ³å…¶", "æ•…å®œ", "ç‹è€…", "è–ä¸Š", "å…ˆç‹", "ä¹ƒæœ‰", "æ³ä¹ƒ", "åˆ¥æœ‰", "ä»Šè€…", "å›ºå®œ", "çš‡ä¸Š", "ä¸”å…¶",
    "å¾’è§€å¤«", "å¸å ¯ä»¥", "å§‹å…¶", "å€è€Œ", "ä¹ƒæ›°", "å‘ä½¿", "æ¼¢æ­¦å¸", "å…ˆæ˜¯", "ä»–æ—¥", "ä¹ƒå‘½", "è§€ä¹", "åœ‹å®¶ä»¥",
    "å¢¨å­", "å€Ÿå¦‚", "è¶³ä»¥", "ä¸Šä¹ƒ", "å—šå‘¼", "æ˜”ä¼Š", "å…ˆè³¢", "é‚ä½¿", "è±ˆæ¯”å¤«", "å›ºå…¶", "æ³æœ‰", "é­¯æ­ç‹", "çš‡å®¶",
    "å¾å›æ˜¯æ™‚", "çŸ¥", "å‘¨ç©†ç‹", "å‰‡æœ‰", "æ˜¯ç”¨", "ä¹ƒè¨€æ›°", "åŠ", "æ•…å¤«", "çŸ§ä¹", "å¤«ä»¥", "å¯§ä»¤", "å¦‚", "ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ", "é‚", "æ‚²å¤«", "å®‰å¾—", "æ•…å¾—", "ä¸”è¦‹å…¶", "æ˜¯ä½•", "è«ä¸", "å£«æœ‰", "çŸ¥å…¶", "æœªè‹¥"
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰"]

def clean_sentence(text):
    """
    æ¸…ç†å¥å­ä¸­çš„åœç”¨è©å‰ç¶´å’Œå¾Œç¶´ã€‚

    åƒæ•¸ï¼š
    text (str): éœ€è¦æ¸…ç†çš„å¥å­ã€‚

    è¿”å›å€¼ï¼š
    str: æ¸…ç†å¾Œçš„å¥å­ã€‚
    """
    for prefix in PREFIX_EXCLUDE:  # éæ­·å‰ç¶´åœç”¨è©åˆ—è¡¨ã€‚
        if text.startswith(prefix):  # å¦‚æœå¥å­ä»¥æŸå€‹å‰ç¶´åœç”¨è©é–‹å§‹ã€‚
            text = text[len(prefix):]  # ç§»é™¤è©²å‰ç¶´ã€‚
            break  # æ‰¾åˆ°åŒ¹é…çš„å‰ç¶´å¾Œï¼Œè·³å‡ºè¿´åœˆï¼Œé¿å…é‡è¤‡è™•ç†ã€‚
    for suffix in SUFFIX_EXCLUDE:  # éæ­·å¾Œç¶´åœç”¨è©åˆ—è¡¨ã€‚
        if text.endswith(suffix):  # å¦‚æœå¥å­ä»¥æŸå€‹å¾Œç¶´åœç”¨è©çµæŸã€‚
            text = text[:-len(suffix)]  # ç§»é™¤è©²å¾Œç¶´ã€‚
            break  # æ‰¾åˆ°åŒ¹é…çš„å¾Œç¶´å¾Œï¼Œè·³å‡ºè¿´åœˆã€‚
    return text.strip()  # ç§»é™¤å¥å­é¦–å°¾çš„ç©ºç™½å­—å…ƒï¼Œä¸¦è¿”å›æ¸…ç†å¾Œçš„å¥å­ã€‚

# ========== è¼‰å…¥èˆ‡æ¸…æ´—åŸå§‹ parsed_results ==========

def load_parsed_results_to_df(json_path):
    """
    å¾ JSON æª”æ¡ˆè¼‰å…¥è§£æå¾Œçš„å¥å­ï¼Œä¸¦å°‡å…¶è½‰æ›ç‚º Pandas DataFrameã€‚

    åƒæ•¸ï¼š
    json_path (Path): JSON æª”æ¡ˆçš„è·¯å¾‘ã€‚

    è¿”å›å€¼ï¼š
    list: åŒ…å«æ¸…ç†å¾Œå¥å­çš„åˆ—è¡¨ã€‚
    """
    print("ğŸ”„ï¸ æ­£åœ¨è¼‰å…¥åŸå¥è³‡æ–™...")  # å°å‡ºè¼‰å…¥è³‡æ–™çš„æç¤ºè¨Šæ¯ã€‚
    with open(json_path, encoding="utf-8") as f:  # é–‹å•Ÿ JSON æª”æ¡ˆã€‚
        parsed_data = json.load(f)  # å°‡ JSON æª”æ¡ˆçš„å…§å®¹è¼‰å…¥åˆ° parsed_data è®Šæ•¸ä¸­ã€‚
    records = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜æ¸…ç†å¾Œçš„å¥å­ã€‚
    for article in parsed_data:  # éæ­·è§£æå¾Œçš„æ–‡ç« è³‡æ–™ã€‚
        for paragraph in article["æ®µè½"]:  # éæ­·æ–‡ç« ä¸­çš„æ¯å€‹æ®µè½ã€‚
            for group in paragraph["å¥çµ„"]:  # éæ­·æ®µè½ä¸­çš„æ¯å€‹å¥çµ„ã€‚
                for sentence in group["å¥å­"]:  # éæ­·å¥çµ„ä¸­çš„æ¯å€‹å¥å­ã€‚
                    records.append(clean_sentence(sentence["å…§å®¹"]))  # æ¸…ç†å¥å­å…§å®¹ï¼Œä¸¦å°‡å…¶æ·»åŠ åˆ° records åˆ—è¡¨ä¸­ã€‚
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} å¥åŸæ–‡å¥å­ã€‚")  # å°å‡ºè¼‰å…¥å®Œæˆçš„è¨Šæ¯ï¼Œä¸¦é¡¯ç¤ºè¼‰å…¥çš„å¥å­æ•¸é‡ã€‚
    return records  # è¿”å›åŒ…å«æ¸…ç†å¾Œå¥å­çš„åˆ—è¡¨ã€‚

# ========== è¼‰å…¥ä¸¦åˆ‡åˆ† compared_text ==========

def load_and_clean_compared_sentences(folder_path, chars_to_remove):
    """
    å¾æŒ‡å®šè³‡æ–™å¤¾è¼‰å…¥å¾…æ¯”å°çš„æ–‡æœ¬æª”æ¡ˆï¼Œä¸¦å°‡å…¶åˆ‡åˆ†ç‚ºå¥å­ã€‚

    åƒæ•¸ï¼š
    folder_path (Path): åŒ…å«æ–‡æœ¬æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ã€‚
    chars_to_remove (str): ç”¨æ–¼åˆ‡åˆ†å¥å­çš„å­—å…ƒã€‚

    è¿”å›å€¼ï¼š
    list: åŒ…å«æ¸…ç†å¾Œå¥å­çš„åˆ—è¡¨ã€‚
    """
    print("ğŸ”„ï¸ æ­£åœ¨è¼‰å…¥æ¯”å°æ–‡æœ¬çš„å¥å­...")  # å°å‡ºè¼‰å…¥æ¯”å°æ–‡æœ¬çš„å¥å­çš„æç¤ºè¨Šæ¯ã€‚
    compared_sentences = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜å¾…æ¯”å°çš„å¥å­ã€‚
    split_pattern = "[" + re.escape(chars_to_remove) + "]"  # å‰µå»ºä¸€å€‹æ­£è¦è¡¨é”å¼æ¨¡å¼ï¼Œç”¨æ–¼åŒ¹é…éœ€è¦ç§»é™¤çš„å­—å…ƒã€‚

    # ä½¿ç”¨ rglob éè¿´æœå°‹æ‰€æœ‰å­è³‡æ–™å¤¾ä¸­çš„ .txt æª”æ¡ˆ
    for file_path in folder_path.rglob("*.txt"):  # éæ­·è³‡æ–™å¤¾åŠå…¶å­è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ .txt æª”æ¡ˆã€‚
        with open(file_path, encoding="utf-8") as f:  # é–‹å•Ÿæ–‡æœ¬æª”æ¡ˆã€‚
            text = f.read()  # è®€å–æª”æ¡ˆå…§å®¹ã€‚
        raw_sentences = re.split(split_pattern, text)  # ä½¿ç”¨æ­£è¦è¡¨é”å¼å°‡æ–‡æœ¬åˆ‡åˆ†ç‚ºå¥å­ã€‚
        cleaned = [clean_sentence(s.strip()) for s in raw_sentences if s.strip()]  # æ¸…ç†æ¯å€‹å¥å­ï¼Œä¸¦æ’é™¤ç©ºç™½å¥å­ã€‚
        compared_sentences.extend(cleaned)  # å°‡æ¸…ç†å¾Œçš„å¥å­æ·»åŠ åˆ° compared_sentences åˆ—è¡¨ä¸­ã€‚
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(compared_sentences)} å¥å¾…æ¯”å°å¥å­ã€‚")  # å°å‡ºè¼‰å…¥å®Œæˆçš„è¨Šæ¯ï¼Œä¸¦é¡¯ç¤ºè¼‰å…¥çš„å¥å­æ•¸é‡ã€‚
    return compared_sentences  # è¿”å›åŒ…å«æ¸…ç†å¾Œå¥å­çš„åˆ—è¡¨ã€‚

# ========== æ§‹å»ºè©è¡¨èˆ‡å‘é‡åŒ– ==========

def build_vocab(all_tokens):
    """
    æ§‹å»ºè©å½™è¡¨ï¼Œå°‡æ¯å€‹è©å½™æ˜ å°„åˆ°ä¸€å€‹å”¯ä¸€çš„ç´¢å¼•ã€‚

    åƒæ•¸ï¼š
    all_tokens (list): åŒ…å«æ‰€æœ‰å¥å­ä¸­æ‰€æœ‰è©å½™çš„åˆ—è¡¨ã€‚

    è¿”å›å€¼ï¼š
    dict: ä¸€å€‹å­—å…¸ï¼Œå°‡æ¯å€‹è©å½™æ˜ å°„åˆ°ä¸€å€‹å”¯ä¸€çš„ç´¢å¼•ã€‚
    """
    vocab = set()  # ä½¿ç”¨é›†åˆ(set)ä¾†å„²å­˜è©å½™ï¼Œä»¥ç¢ºä¿æ¯å€‹è©å½™åªå‡ºç¾ä¸€æ¬¡ã€‚
    for tokens in all_tokens:  # éæ­·æ‰€æœ‰å¥å­ä¸­çš„è©å½™åˆ—è¡¨ã€‚
        vocab.update(tokens)  # å°‡å¥å­ä¸­çš„è©å½™æ·»åŠ åˆ°è©å½™é›†åˆä¸­ã€‚
    vocab = sorted(vocab)  # å°‡è©å½™é›†åˆæ’åºï¼Œä»¥ç¢ºä¿è©å½™çš„é †åºä¸€è‡´ã€‚
    return {word: idx for idx, word in enumerate(vocab)}  # å‰µå»ºä¸¦è¿”å›è©å½™åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸ã€‚

def vectorize_tokens(tokens_list, word2idx):
    """
    å°‡è©å½™åˆ—è¡¨è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºã€‚

    åƒæ•¸ï¼š
    tokens_list (list): åŒ…å«è©å½™åˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯å€‹è©å½™åˆ—è¡¨å°æ‡‰ä¸€å€‹å¥å­ã€‚
    word2idx (dict): è©å½™åˆ°ç´¢å¼•çš„æ˜ å°„å­—å…¸ã€‚

    è¿”å›å€¼ï¼š
    torch.Tensor: ä¸€å€‹ PyTorch å¼µé‡ï¼Œå…¶ä¸­åŒ…å«å¥å­çš„å‘é‡è¡¨ç¤ºã€‚
    """
    vectors = torch.zeros((len(tokens_list), len(word2idx)), device=DEVICE)  # å‰µå»ºä¸€å€‹å…¨é›¶å¼µé‡ï¼Œç”¨æ–¼å„²å­˜å‘é‡è¡¨ç¤ºã€‚
    for i, tokens in enumerate(tokens_list):  # éæ­·æ¯å€‹å¥å­çš„è©å½™åˆ—è¡¨ã€‚
        for token in tokens:  # éæ­·å¥å­ä¸­çš„æ¯å€‹è©å½™ã€‚
            if token in word2idx:  # å¦‚æœè©å½™åœ¨è©å½™è¡¨ä¸­ã€‚
                vectors[i, word2idx[token]] = 1  # å°‡å°æ‡‰ç´¢å¼•ä½ç½®çš„å€¼è¨­ç‚º 1ï¼Œè¡¨ç¤ºè©²è©å½™åœ¨å¥å­ä¸­å‡ºç¾ã€‚
    return vectors  # è¿”å›åŒ…å«å‘é‡è¡¨ç¤ºçš„å¼µé‡ã€‚

# ========== è¨ˆç®— Batch Jaccard ==========

def batch_jaccard(compared_vecs, origin_vecs):
    """
    ä½¿ç”¨ GPU åŠ é€Ÿè¨ˆç®—æ‰¹æ¬¡ Jaccard ç›¸ä¼¼åº¦ã€‚

    åƒæ•¸ï¼š
    compared_vecs (torch.Tensor): å¾…æ¯”å°å¥å­çš„å‘é‡è¡¨ç¤ºã€‚
    origin_vecs (torch.Tensor): åŸå§‹å¥å­çš„å‘é‡è¡¨ç¤ºã€‚

    è¿”å›å€¼ï¼š
    torch.Tensor: åŒ…å« Jaccard ç›¸ä¼¼åº¦å€¼çš„çŸ©é™£ã€‚
    """
    intersection = torch.matmul(compared_vecs, origin_vecs.T)  # è¨ˆç®—äº¤é›†å¤§å°ï¼Œä½¿ç”¨çŸ©é™£ä¹˜æ³•ã€‚
    compared_sum = compared_vecs.sum(dim=1, keepdim=True)  # è¨ˆç®—å¾…æ¯”å°å¥å­å‘é‡çš„å…ƒç´ å’Œã€‚
    origin_sum = origin_vecs.sum(dim=1, keepdim=True).T  # è¨ˆç®—åŸå§‹å¥å­å‘é‡çš„å…ƒç´ å’Œã€‚
    union = compared_sum + origin_sum - intersection + 1e-9 # è¨ˆç®—è¯é›†å¤§å°ï¼ŒåŠ å…¥ä¸€å€‹å°çš„å¹³æ»‘é …ä»¥é¿å…é™¤ä»¥é›¶çš„é¢¨éšªã€‚
    jaccard = intersection / union  # è¨ˆç®— Jaccard ç›¸ä¼¼åº¦ã€‚
    return jaccard  # è¿”å› Jaccard ç›¸ä¼¼åº¦çŸ©é™£ã€‚

# ========== ä¸»ç¨‹å¼ ==========

def main():
    # è¼‰å…¥è³‡æ–™
    origin_sentences = load_parsed_results_to_df(PARSED_RESULTS_PATH)  # è¼‰å…¥åŸå§‹å¥å­ã€‚
    compared_sentences = load_and_clean_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)  # è¼‰å…¥ä¸¦æ¸…ç†å¾…æ¯”å°å¥å­ã€‚

    # CKIP åˆ†è©
    print("ğŸªš åˆ†è©è™•ç†...")  # å°å‡ºåˆ†è©è™•ç†çš„æç¤ºè¨Šæ¯ã€‚
    # åˆå§‹åŒ– CKIP æ–·è©å™¨ï¼Œä¸¦æŒ‡å®šè£ç½®
    ws_driver = CkipWordSegmenter(device=CKIP_DEVICE, model="bert-base")
    origin_tokens = ws_driver(origin_sentences)  # å°åŸå§‹å¥å­é€²è¡Œæ–·è©ã€‚
    compared_tokens = ws_driver(compared_sentences)  # å°å¾…æ¯”å°å¥å­é€²è¡Œæ–·è©ã€‚

    # æ§‹å»ºè©è¡¨å’Œå‘é‡åŒ–
    print("â¡ï¸ å‘é‡åŒ–...")  # å°å‡ºå‘é‡åŒ–è™•ç†çš„æç¤ºè¨Šæ¯ã€‚
    word2idx = build_vocab(origin_tokens + compared_tokens)  # æ§‹å»ºåŒ…å«æ‰€æœ‰è©å½™çš„è©å½™è¡¨ã€‚
    origin_vecs = vectorize_tokens(origin_tokens, word2idx)  # å°‡åŸå§‹å¥å­è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºã€‚
    compared_vecs = vectorize_tokens(compared_tokens, word2idx)  # å°‡å¾…æ¯”å°å¥å­è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºã€‚

    # æ‰¹æ¬¡è¨ˆç®— Jaccard ç›¸ä¼¼åº¦ (å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨)
    print("ğŸ§¬ è¨ˆç®— Jaccard ç›¸ä¼¼åº¦...")  # å°å‡ºè¨ˆç®— Jaccard ç›¸ä¼¼åº¦çš„æç¤ºè¨Šæ¯ã€‚
    start_time = time.time()  # ç´€éŒ„é–‹å§‹æ™‚é–“
    batch_size = 512  # è¨­å®šæ‰¹æ¬¡å¤§å°ï¼Œæ‚¨å¯ä»¥æ ¹æ“š GPU è¨˜æ†¶é«”æƒ…æ³èª¿æ•´é€™å€‹å€¼
    num_compared = compared_vecs.size(0)  # å–å¾—å¾…æ¯”å°å¥å­çš„ç¸½æ•¸é‡
    all_jaccard_matrices = []  # åˆå§‹åŒ–ä¸€å€‹åˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜æ¯å€‹æ‰¹æ¬¡çš„ Jaccard ç›¸ä¼¼åº¦çŸ©é™£

    for i in tqdm(range(0, num_compared, batch_size), desc="Jaccard Batches"):
        # ç²å–ç•¶å‰æ‰¹æ¬¡çš„å¾…æ¯”å°å¥å­å‘é‡
        compared_batch = compared_vecs[i:i + batch_size]
        try:
            # è¨ˆç®—ç•¶å‰æ‰¹æ¬¡èˆ‡æ‰€æœ‰åŸå§‹å¥å­ä¹‹é–“çš„ Jaccard ç›¸ä¼¼åº¦
            jaccard_batch = batch_jaccard(compared_batch, origin_vecs)
            # å°‡ç•¶å‰æ‰¹æ¬¡çš„çµæœç§»å› CPU å„²å­˜ï¼Œä»¥æ¸›å°‘ GPU è¨˜æ†¶é«”å£“åŠ›
            all_jaccard_matrices.append(jaccard_batch.cpu())
        except torch.cuda.OutOfMemoryError as e:
            print(f"âš ï¸ GPU è¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤ï¼š{e}")
            print(f"å˜—è©¦ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°ï¼š{batch_size // 2}")
            batch_size //= 2  # æ¸›å°æ‰¹æ¬¡å¤§å°
            if batch_size == 0:
                raise RuntimeError("æ‰¹æ¬¡å¤§å°å·²é™è‡³ 0ï¼Œä½†ä»ç„¶ç™¼ç”Ÿè¨˜æ†¶é«”ä¸è¶³éŒ¯èª¤ã€‚è«‹æª¢æŸ¥æ‚¨çš„ GPU æˆ–è€ƒæ…®åœ¨ CPU ä¸Šé‹è¡Œã€‚") from e
            compared_batch = compared_vecs[i:i + batch_size]
            jaccard_batch = batch_jaccard(compared_batch, origin_vecs)
            all_jaccard_matrices.append(jaccard_batch.cpu())
        except Exception as e:
            print(f"âš ï¸ è¨ˆç®— Jaccard ç›¸ä¼¼åº¦æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            # å¯ä»¥åœ¨é€™è£¡åŠ å…¥éŒ¯èª¤è™•ç†é‚è¼¯ï¼Œä¾‹å¦‚è·³éç•¶å‰æ‰¹æ¬¡æˆ–å„²å­˜éŒ¯èª¤æ—¥èªŒ
            continue

    # å°‡æ‰€æœ‰æ‰¹æ¬¡çš„ Jaccard ç›¸ä¼¼åº¦çŸ©é™£åœ¨åˆ—çš„æ–¹å‘ä¸Šæ‹¼æ¥èµ·ä¾†
    jaccard_matrix = torch.cat(all_jaccard_matrices, dim=0).to(DEVICE)
    end_time = time.time()  # ç´€éŒ„çµæŸæ™‚é–“
    print(f"Jaccard ç›¸ä¼¼åº¦è¨ˆç®—å®Œæˆï¼Œè€—æ™‚ï¼š{end_time - start_time:.2f} ç§’")

    # æ‰¾æœ€ä½³åŒ¹é…
    print("ğŸ’¹ å°‹æ‰¾æœ€ä½³åŒ¹é…...")  # å°å‡ºå°‹æ‰¾æœ€ä½³åŒ¹é…çš„æç¤ºè¨Šæ¯ã€‚
    matches = []  # åˆå§‹åŒ–ä¸€å€‹ç©ºåˆ—è¡¨ï¼Œç”¨æ–¼å„²å­˜åŒ¹é…çµæœã€‚
    best_scores, best_indices = jaccard_matrix.max(dim=1)  # æ‰¾åˆ°æ¯å€‹å¾…æ¯”å°å¥å­èˆ‡åŸå§‹å¥å­ä¹‹é–“çš„æœ€å¤§ Jaccard ç›¸ä¼¼åº¦ï¼Œä»¥åŠå°æ‡‰çš„åŸå§‹å¥å­ç´¢å¼•ã€‚
    # best_scores: æ¯å€‹å¾…æ¯”å°å¥å­ï¼Œèˆ‡æ‰€æœ‰ origin_sentences æ¯”å°å¾Œï¼Œæœ€é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•¸
    # best_indices: æ¯å€‹å¾…æ¯”å°å¥å­ï¼Œèˆ‡æ‰€æœ‰ origin_sentences æ¯”å°å¾Œï¼Œæœ€é«˜çš„ç›¸ä¼¼åº¦åˆ†æ•¸æ‰€å°æ‡‰çš„ origin_sentences çš„ index

    for idx, (score, best_idx) in enumerate(zip(best_scores.tolist(), best_indices.tolist())):  # éæ­·æ¯å€‹å¾…æ¯”å°å¥å­åŠå…¶æœ€ä½³åŒ¹é…çµæœã€‚
        if score >= JACCARD_THRESHOLD:  # å¦‚æœ Jaccard ç›¸ä¼¼åº¦å¤§æ–¼æˆ–ç­‰æ–¼é–¾å€¼ã€‚
            matches.append({  # å°‡åŒ¹é…çµæœæ·»åŠ åˆ° matches åˆ—è¡¨ä¸­ã€‚
                "Comparedå¥å­": compared_sentences[idx],  # å¾…æ¯”å°å¥å­å…§å®¹ã€‚
                "å°æ‡‰åŸå¥": origin_sentences[best_idx],  # åŒ¹é…åˆ°çš„åŸå§‹å¥å­å…§å®¹ã€‚
                "Jaccardç›¸ä¼¼åº¦": score  # Jaccard ç›¸ä¼¼åº¦å€¼ã€‚
            })

    # åŒ¯å‡ºçµæœç‚º JSON
    with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:  # é–‹å•Ÿ JSON æª”æ¡ˆã€‚
        json.dump(matches, f, ensure_ascii=False, indent=2)  # å°‡åŒ¹é…çµæœä»¥ JSON æ ¼å¼å¯«å…¥æª”æ¡ˆã€‚

    print(f"\nâœ… æ¯”å°å®Œæˆï¼å…±å„²å­˜ {len(matches)} ç­†çµæœã€‚å·²è¼¸å‡ºåˆ° {OUTPUT_JSON_PATH}")  # å°å‡ºæ¯”å°å®Œæˆçš„è¨Šæ¯ï¼Œä¸¦é¡¯ç¤ºå„²å­˜çµæœçš„æª”æ¡ˆè·¯å¾‘ã€‚

if __name__ == "__main__":
    main()  # åŸ·è¡Œä¸»ç¨‹å¼ã€‚