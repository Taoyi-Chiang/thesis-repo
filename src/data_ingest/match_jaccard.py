import json
import re
from pathlib import Path
from tqdm import tqdm
import torch
import gc
from torch.utils.data import DataLoader, TensorDataset
import time
from ckip_transformers.nlp import CkipWordSegmenter
import cupy as cp  # GPU acceleration with CuPy

# ========== ä½¿ç”¨è€…è¨­å®š (ä¸€è™•å¯æ§) ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/compared_text/è«¸å­")
OUTPUT_JSON_PATH = Path(r"D:/lufu_allusion/data/processed/ZI_match_results_jaccard.json")
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\#\\-\\ï¼\\(\\)\\[\\]\\]\\/(),1234567890Â¶"
JACCARD_THRESHOLD = 0.7       # ç›¸ä¼¼åº¦é–¾å€¼ï¼Œå¯èª¿æ•´
BATCH_SIZE = 4096             # ç”¨æˆ¶å¯èª¿æ•´çš„æ‰¹æ¬¡å¤§å°
MIN_BATCH_SIZE = 512          # æœ€å°æ‰¹æ¬¡å¤§å°
# ORIGIN_CHUNK_SIZE will be set dynamically after tokenization
ORIGIN_CHUNK_SIZE = None      # placeholder

# ========== è¨­å‚™æª¢æ¸¬ ==========
USE_GPU = True
if USE_GPU and cp.cuda.runtime.getDeviceCount() > 0:
    DEVICE = 'cuda'
    print(f"âœ… ä½¿ç”¨ GPU åŠ é€Ÿ (CuPy Devices={cp.cuda.runtime.getDeviceCount()})")
else:
    DEVICE = 'cpu'
    USE_GPU = False
    print("âš ï¸ æœªåµæ¸¬åˆ°å¯ç”¨ GPUï¼Œé€€å› CPU é‹ç®—ã€‚")

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [
    # åœç”¨è©åˆ—è¡¨ç•¥
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰", "ä¹", "ç„‰"]

def clean_sentence(text):
    for prefix in PREFIX_EXCLUDE:
        if text.startswith(prefix):
            return text[len(prefix):].strip()
    for suffix in SUFFIX_EXCLUDE:
        if text.endswith(suffix):
            return text[:-len(suffix)].strip()
    return text.strip()

# ========== è®€å–èˆ‡æ¸…æ´—è³‡æ–™ ==========
def load_parsed_results(json_path):
    print("ğŸ”„ï¸ è¼‰å…¥åŸå¥è³‡æ–™...")
    with open(json_path, encoding="utf-8") as f:
        data = json.load(f)
    records = []
    for article in data:
        for para in article.get("æ®µè½", []):
            for group in para.get("å¥çµ„", []):
                for sent in group.get("å¥å­", []):
                    clean = clean_sentence(sent.get("å…§å®¹", ""))
                    if clean:
                        records.append({
                            "article_num": article.get("ç¯‡è™Ÿ"),
                            "author": article.get("è³¦å®¶", ""),
                            "article_title": article.get("è³¦ç¯‡", ""),
                            "paragraph_num": para.get("æ®µè½ç·¨è™Ÿ"),
                            "group_num": group.get("å¥çµ„ç·¨è™Ÿ"),
                            "sentence_num": sent.get("å¥ç·¨è™Ÿ"),
                            "original": sent.get("å…§å®¹", ""),
                            "cleaned": clean
                        })
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} æ¢åŸå¥ã€‚")
    return records


def load_compared_sentences(folder_path, chars_to_remove):
    print("ğŸ”„ï¸ è¼‰å…¥æ¯”å°å¥è³‡æ–™...")
    pattern = "[" + re.escape(chars_to_remove) + "]"
    sents = []
    for fp in folder_path.rglob("*.txt"):
        raw = fp.read_text(encoding="utf-8").replace("\n", "")
        raw = re.sub(r"<[^>]*>", "", raw)  # åˆªé™¤æ‰€æœ‰ <...> æ¨™è¨˜
        for idx, seg in enumerate(re.split(pattern, raw)):
            clean = clean_sentence(seg)
            if clean:
                sents.append({
                    "matched_file": fp.parent.name + "/" + fp.stem,
                    "matched_index": idx,
                    "raw": seg,
                    "cleaned": clean
                })
    print(f"âœ… è¼‰å…¥å®Œæˆï¼Œå…± {len(sents)} æ¢å¾…æ¯”å°å¥ã€‚")
    return sents

# ========== åˆ†è© ==========
def segment_in_batches(sentences, segmenter, batch_size=100, text_type=""):
    print(f"ğŸªš åˆ†è©è™•ç†ï¼š{text_type}ï¼ˆå…± {len(sentences)} æ¢ï¼‰...")
    all_tokens = []
    with tqdm(total=len(sentences), desc=f"åˆ†è© ({text_type})") as pbar:
        for i in range(0, len(sentences), batch_size):
            batch = [s["cleaned"] for s in sentences[i:i+batch_size]]
            toks = segmenter(batch, show_progress=False)
            all_tokens.extend(toks)
            pbar.update(len(batch))
            del toks, batch
            torch.cuda.empty_cache(); gc.collect()
    print(f"âœ… {text_type} åˆ†è©å®Œæˆã€‚")
    return all_tokens

# ========== è©å½™è¡¨å»ºç«‹ ==========
def build_vocab(tokens_list):
    print("â¡ï¸ å»ºæ§‹è©å½™è¡¨...")
    vocab = sorted({w for toks in tokens_list for w in toks})
    print(f"âœ… è©å½™è¡¨å¤§å°ï¼š{len(vocab)} å€‹è©ã€‚")
    return {w: i for i, w in enumerate(vocab)}

# ========== å‘é‡åŒ–åˆ° GPU åˆ†å¡Š ==========
def tokens_to_gpu_matrix(tokens_list, word2idx, chunk_size):
    mats = []
    for st in range(0, len(tokens_list), chunk_size):
        ed = min(st + chunk_size, len(tokens_list))
        mat = cp.zeros((ed - st, len(word2idx)), dtype=cp.int8)
        for i, toks in enumerate(tokens_list[st:ed]):
            for w in toks:
                idx = word2idx.get(w)
                if idx is not None:
                    mat[i, idx] = 1
        mats.append(mat)
    return mats

# ========== æ‰¹æ¬¡ Jaccard è¨ˆç®— (CuPy) ==========
def batch_jaccard_gpu(comp_mat, origin_mats):
    best_s = cp.zeros(comp_mat.shape[0], dtype=cp.float32)
    best_i = cp.zeros(comp_mat.shape[0], dtype=cp.int32)
    for idx, om in enumerate(origin_mats):
        inter = comp_mat.dot(om.T)
        sc = comp_mat.sum(axis=1, keepdims=True)
        so = om.sum(axis=1, keepdims=True).T
        jac = inter / (sc + so - inter + 1e-9)
        mv = jac.max(axis=1)
        mi = jac.argmax(axis=1) + idx * om.shape[0]
        mask = mv > best_s
        best_s[mask] = mv[mask]
        best_i[mask] = mi[mask]
        del inter, sc, so, jac
        cp.get_default_memory_pool().free_all_blocks()
    return best_s, best_i

# ========== ä¸»ç¨‹å¼ ==========
def main():
    origin_data = load_parsed_results(PARSED_RESULTS_PATH)
    compared_data = load_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)

    # åˆ†è©
    ws = CkipWordSegmenter(device=0 if USE_GPU else -1, model="bert-base")
    origin_tokens = segment_in_batches(origin_data, ws, batch_size=100, text_type="åŸå§‹æ–‡æœ¬")
    compared_tokens = segment_in_batches(compared_data, ws, batch_size=100, text_type="æ¯”å°æ–‡æœ¬")
    del ws; torch.cuda.empty_cache(); gc.collect()

    # å»ºè©è¡¨
    word2idx = build_vocab(origin_tokens)

        # å‹•æ…‹è¨­å®šåˆ†å¡Šå¤§å°
    ORIGIN_CHUNK_SIZE = len(origin_tokens)

    # å‘é‡åŒ– origin
    if USE_GPU:
        print(f"ğŸ”¢ å°‡åŸå¥ tokens å‘é‡åŒ–ä¸¦åˆ‡å¡Šä¸Š GPU (chunk_size={ORIGIN_CHUNK_SIZE})...")
        origin_mats = tokens_to_gpu_matrix(origin_tokens, word2idx, ORIGIN_CHUNK_SIZE)
    else:
        origin_vecs_cpu = vectorize_tokens(origin_tokens, word2idx, device=torch.device('cpu'))

    # Jaccard & åŒ¹é…
    matches = []
    total = len(compared_data)
    print("ğŸ§ª Jaccard & åŒ¹é…... ")
    for st in tqdm(range(0, total, BATCH_SIZE), desc="Jaccard & åŒ¹é…"):
        ed = min(st + BATCH_SIZE, total)
        batch_t = compared_tokens[st:ed]
        if USE_GPU:
            comp_mat = tokens_to_gpu_matrix(batch_t, word2idx, BATCH_SIZE)[0]
            sc, ix = batch_jaccard_gpu(comp_mat, origin_mats)
            sc, ix = cp.asnumpy(sc), cp.asnumpy(ix)
            cp.get_default_memory_pool().free_all_blocks()
        else:
            comp_vecs = vectorize_tokens(batch_t, word2idx, device=torch.device('cpu'))
            jacc = batch_jaccard_cpu(comp_vecs, origin_vecs_cpu)
            sc, ix = jacc.max(1).numpy(), jacc.argmax(1).numpy()
            del comp_vecs, jacc

        for i, (s, oid) in enumerate(zip(sc, ix)):
            if s >= JACCARD_THRESHOLD:
                od = origin_data[oid]; cd = compared_data[st + i]
                matches.append({
                    **{k: od[k] for k in ["article_num","author","article_title","paragraph_num","group_num","sentence_num","original"]},
                    **{"matched_file": cd["matched_file"],"matched_index": cd["matched_index"],"matched": cd["raw"],"similarity": float(s)}
                })

    print(f"âœ… åŒ¹é…å®Œæˆï¼Œå…± {len(matches)} ç­†çµæœã€‚")
    print("ğŸ“„ æ’åºä¸¦è¼¸å‡º JSON...")
    matches = sorted(matches, key=lambda x: (
        int(x["article_num"]), int(x["paragraph_num"]), int(x["group_num"]), int(x["sentence_num"])  
    ))
    with OUTPUT_JSON_PATH.open("w", encoding="utf-8") as f:
        json.dump(matches, f, ensure_ascii=False, indent=2)
    print(f"âœ… å®Œæˆï¼è¼¸å‡ºæª”æ¡ˆï¼š{OUTPUT_JSON_PATH}")

if __name__ == "__main__":
    main()
