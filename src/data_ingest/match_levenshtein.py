# sentence_word_match_pipeline.py (å¥å­Jaccardæ¯”å° + è©å½™è£œæ•‘ + æ¨™æº–å·®åˆªé™¤ + åœç”¨è©æ¸…ç† + Bigramæ‰¾è© + æ¸…é™¤æ¨™è¨˜)

import json
import re
from pathlib import Path
from collections import Counter
import numpy as np
from tqdm import tqdm
import torch
from ckip_transformers.nlp import CkipWordSegmenter

# ========== ä½¿ç”¨è€…è¨­å®š ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")
COMPARED_FOLDER_PATH = Path(r"D:/lufu_allusion/data/raw/æ¯›è©©")
OUTPUT_JSON_PATH = Path(r"D:/lufu_allusion/data/processed/sample_match_results_sentence_word.json")
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€ã€”ã€•ã€ã€ã€Šã€‹ã€ˆã€‰\\(\\)\\[\\]/,1234567890Â¶\\-"
JACCARD_THRESHOLD = 0.45
WORD_MIN_LENGTH = 2
WORD_MAX_LENGTH = 5
STD_THRESHOLD_K = 2

# ========== åœç”¨è©è¨­å®š ==========
PREFIX_EXCLUDE = [
    "å¾’è§€å…¶","çŸ§å¤«","çŸ§ä¹ƒ","è‡³å¤«","æ‡¿å¤«","è“‹ç”±æˆ‘å›","é‡æ›°","æ˜¯çŸ¥","å¤«å…¶","æ‡¿å…¶","æ‰€ä»¥",
    "æƒ³å¤«","å…¶å§‹ä¹Ÿ","ç•¶å…¶","æ³å¾©","æ™‚å‰‡","è‡³è‹¥","è±ˆç¨","è‹¥ä¹ƒ","ä»Šå‰‡","ä¹ƒçŸ¥","æ—¢è€Œ","å—Ÿä¹",
    "æ•…æˆ‘å","è§€å¤«","ç„¶è€Œ","çˆ¾ä¹ƒ","æ˜¯ä»¥","åŸå¤«","æ›·è‹¥","æ–¯å‰‡","æ–¼æ™‚","æ–¹ä»Š","äº¦ä½•å¿…","è‹¥ç„¶",
    "å®¢æœ‰","è‡³æ–¼","å‰‡çŸ¥","ä¸”å¤«","æ–¯ä¹ƒ","æ³","æ–¼æ˜¯","è¦©å¤«","ä¸”å½¼","è±ˆè‹¥","å·²è€Œ","å§‹ä¹Ÿ","æ•…",
    "ç„¶å‰‡","è±ˆå¦‚æˆ‘","è±ˆä¸ä»¥","æˆ‘åœ‹å®¶","å…¶å·¥è€…","æ‰€è¬‚","ä»Šå¾å›","åŠå¤«","çˆ¾å…¶","å°‡ä»¥","å¯ä»¥",
    "ä»Š","åœ‹å®¶","ç„¶å¾Œ","å‘éæˆ‘å","å‰‡æœ‰","å½¼","æƒœä¹","ç”±æ˜¯","ä¹ƒè¨€æ›°","è‹¥å¤«","äº¦ä½•ç”¨","ä¸ç„¶",
    "å˜‰å…¶","ä»Šå‰‡","å¾’ç¾å¤«","æ•…èƒ½","æœ‰æ¢è€…æ›°","æƒœå¦‚","è€Œæ³","é€®å¤«","èª å¤«","æ–¼æˆ²","æ´ä¹","ä¼Šæ˜”",
    "å‰‡å°‡","ä»Šå‰‡","æ³ä»Š","å£«æœ‰","æš¨ä¹","äº¦ä½•è¾¨å¤«","ä¿¾å¤«","äº¦çŒ¶","ç»å¤«","æ™‚ä¹Ÿ","å›ºçŸ¥","è¶³ä»¥",
    "çŸ§åœ‹å®¶","æ¯”ä¹","äº¦ç”±","è§€å…¶","å°‡ä¿¾ä¹","è–äºº","å›å­","æ–¼ä»¥","ä¹ƒ","æ–¯è“‹","å™«","å¤«æƒŸ",
    "é«˜çš‡å¸","å¸æ—¢","å˜‰å…¶","å§‹å‰‡","åˆå®‰å¾—","å…¶","å„’æœ‰","ç•¶æ˜¯æ™‚ä¹Ÿ","å¤«ç„¶","å®œä¹","æ•…å…¶","åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ","ä»Šæˆ‘åœ‹å®¶","æ˜¯æ™‚","æœ‰å¸","å‘è‹¥","æˆ‘çš‡","æ•…ç‹è€…","å‰‡","é„’å­","å­°","æš¨å¤«","ç”¨èƒ½",
    "æ•…å°‡","æ³å…¶","æ•…å®œ","ç‹è€…","è–ä¸Š","å…ˆç‹","ä¹ƒæœ‰","æ³ä¹ƒ","åˆ¥æœ‰","ä»Šè€…","å›ºå®œ","çš‡ä¸Š","ä¸”å…¶",
    "å¾’è§€å¤«","å¸å ¯ä»¥","å§‹å…¶","å€è€Œ","ä¹ƒæ›°","å‘ä½¿","æ¼¢æ­¦å¸","å…ˆæ˜¯","ä»–æ—¥","ä¹ƒå‘½","è§€ä¹","åœ‹å®¶ä»¥",
    "å¢¨å­","å€Ÿå¦‚","è¶³ä»¥","ä¸Šä¹ƒ","å—šå‘¼","æ˜”ä¼Š","å…ˆè³¢","é‚ä½¿","è±ˆæ¯”å¤«","å›ºå…¶","æ³æœ‰","é­¯æ­ç‹","çš‡å®¶",
    "å¾å›æ˜¯æ™‚","çŸ¥","å‘¨ç©†ç‹","å‰‡æœ‰","æ˜¯ç”¨","ä¹ƒè¨€æ›°","åŠ","æ•…å¤«","çŸ§ä¹","å¤«ä»¥","å¯§ä»¤","å¦‚","ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ","é‚","æ‚²å¤«","å®‰å¾—","æ•…å¾—","ä¸”è¦‹å…¶","æ˜¯ä½•","è«ä¸","å£«æœ‰","çŸ¥å…¶","æœªè‹¥"
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰"]

# ========== è£ç½®è¨­å®š ==========
if torch.cuda.is_available():
    DEVICE = torch.device("cuda")
    print(f"âœ… åµæ¸¬åˆ° GPU: {torch.cuda.get_device_name(0)}ï¼Œä½¿ç”¨ GPU åŠ é€Ÿ")
else:
    DEVICE = torch.device("cpu")
    print("âš ï¸ ä½¿ç”¨ CPU é‹ç®—")

# ========== æ¸…æ´—å·¥å…· ==========
def clean_text(text):
    text = re.sub(r"<[^>]*>", "", text)  # ğŸ”¥ æ–°å¢ï¼šåˆªé™¤ <...> æ¨™è¨˜
    text = re.sub(f"[{CHARS_TO_REMOVE}]", "", text)
    return text.strip()

def clean_prefix_suffix(text):
    for prefix in PREFIX_EXCLUDE:
        if text.startswith(prefix):
            text = text[len(prefix):]
            break
    for suffix in SUFFIX_EXCLUDE:
        if text.endswith(suffix):
            text = text[:-len(suffix)]
            break
    return text.strip()

# ========== è¼‰å…¥è³‡æ–™ ==========
def load_parsed_sentences(json_path):
    with open(json_path, encoding="utf-8") as f:
        parsed_data = json.load(f)
    records = []
    for article_idx, article in enumerate(parsed_data):
        article_title = article["è³¦ç¯‡"]
        for paragraph in article["æ®µè½"]:
            for group in paragraph["å¥çµ„"]:
                for sentence in group["å¥å­"]:
                    content = clean_text(sentence["å…§å®¹"])
                    records.append({
                        "order": (article_idx, paragraph["æ®µè½ç·¨è™Ÿ"], group["å¥çµ„ç·¨è™Ÿ"], sentence["å¥ç·¨è™Ÿ"]),
                        "source_id": f"{article_title}_æ®µ{paragraph['æ®µè½ç·¨è™Ÿ']}_å¥çµ„{group['å¥çµ„ç·¨è™Ÿ']}_å¥{sentence['å¥ç·¨è™Ÿ']}",
                        "original": content
                    })
    return records

def load_compared_sentences(folder_path):
    sentences = []
    for file in folder_path.glob("*.txt"):
        with open(file, encoding="utf-8") as f:
            text = f.read()
        lines = [clean_text(line) for line in text.splitlines() if line.strip()]
        for idx, line in enumerate(lines):
            relative_path = file.relative_to(folder_path.parent)
            sentences.append({
                "matched_file": str(relative_path).replace(".txt", ""),
                "matched_index": idx,
                "matched": line
            })
    return sentences

# ========== åˆ†è©èˆ‡å‘é‡åŒ– ==========
def build_vocab(all_tokens):
    vocab = set()
    for tokens in all_tokens:
        vocab.update(tokens)
    vocab = sorted(vocab)
    return {word: idx for idx, word in enumerate(vocab)}

def vectorize_tokens(tokens_list, word2idx):
    vectors = torch.zeros((len(tokens_list), len(word2idx)), device=DEVICE)
    for i, tokens in enumerate(tokens_list):
        for token in tokens:
            if token in word2idx:
                vectors[i, word2idx[token]] = 1
    return vectors

# ========== Jaccard ç›¸ä¼¼åº¦ ==========
def batch_jaccard(compared_vecs, origin_vecs):
    intersection = torch.matmul(compared_vecs, origin_vecs.T)
    compared_sum = compared_vecs.sum(dim=1, keepdim=True)
    origin_sum = origin_vecs.sum(dim=1, keepdim=True).T
    union = compared_sum + origin_sum - intersection
    jaccard = intersection / union
    return jaccard

# ========== æŠ½å–é—œéµè©ï¼ˆå–®è©+Bigramï¼‰ ==========
def extract_keywords(sentences):
    ws_driver = CkipWordSegmenter(model="bert-base", device=0)
    results = ws_driver([s["original"] for s in sentences])
    extracted = []
    for record, tokens in zip(sentences, results):
        clean_tokens = [clean_prefix_suffix(t) for t in tokens if WORD_MIN_LENGTH <= len(t) <= WORD_MAX_LENGTH]
        for token in clean_tokens:
            extracted.append({
                "source_id": record["source_id"],
                "original": record["original"],
                "keyword": token,
                "order": record["order"]
            })
        for i in range(len(clean_tokens) - 1):
            bigram = clean_tokens[i] + clean_tokens[i + 1]
            if WORD_MIN_LENGTH <= len(bigram) <= WORD_MAX_LENGTH:
                extracted.append({
                    "source_id": record["source_id"],
                    "original": record["original"],
                    "keyword": bigram,
                    "order": record["order"]
                })
    return extracted

# ========== æ¨™æº–å·®åˆªé™¤ ==========

def remove_noisy_matches(matches):
    counter = Counter(m["matched"] for m in matches)
    counts = np.array(list(counter.values()))
    mean = counts.mean()
    std = counts.std()
    threshold = mean + STD_THRESHOLD_K * std
    noisy_matched = {kw for kw, cnt in counter.items() if cnt > threshold}
    filtered = [m for m in matches if m["matched"] not in noisy_matched]
    return filtered

# ä¿®æ”¹ match_keywords_to_compared

def match_keywords_to_compared(extracted_keywords, compared_sentences):
    ws_driver = CkipWordSegmenter(model="bert-base", device=0)
    comp_sentences = [c["matched"] for c in compared_sentences]
    comp_tokens_list = ws_driver(comp_sentences)

    compared_token_map = []
    for record, tokens in zip(compared_sentences, comp_tokens_list):
        clean_tokens = [t for t in tokens if WORD_MIN_LENGTH <= len(t) <= WORD_MAX_LENGTH]
        all_units = set(clean_tokens)
        for i in range(len(clean_tokens) - 1):
            bigram = clean_tokens[i] + clean_tokens[i + 1]
            if WORD_MIN_LENGTH <= len(bigram) <= WORD_MAX_LENGTH:
                all_units.add(bigram)
        compared_token_map.append((record, all_units))

    matches = []
    for keyword_record in tqdm(extracted_keywords):
        keyword = keyword_record["keyword"]
        for comp_record, units in compared_token_map:
            if keyword in units:
                matches.append({
                    "source_id": keyword_record["source_id"],
                    "original": keyword_record["original"],
                    "matched_file": comp_record["matched_file"],
                    "matched_index": comp_record["matched_index"],
                    "matched": comp_record["matched"],
                    "similarity": 1.0,
                    "order": keyword_record["order"]
                })
    return matches

# ä¿®æ­£ main()

def main():
    origin_records = load_parsed_sentences(PARSED_RESULTS_PATH)
    compared_records = load_compared_sentences(COMPARED_FOLDER_PATH)
    
    print("\U0001f680 å•Ÿå‹• CKIP åˆ†è©...")
    ws_driver = CkipWordSegmenter(model="bert-base", device=0)
    origin_sentences = [r["original"] for r in origin_records]
    compared_sentences = [c["matched"] for c in compared_records]

    origin_tokens = ws_driver(origin_sentences)
    compared_tokens = ws_driver(compared_sentences)

    word2idx = build_vocab(origin_tokens + compared_tokens)
    origin_vecs = vectorize_tokens(origin_tokens, word2idx)
    compared_vecs = vectorize_tokens(compared_tokens, word2idx)

    print("\U0001f50e è¨ˆç®— Jaccard ç›¸ä¼¼åº¦...")
    jaccard_matrix = batch_jaccard(compared_vecs, origin_vecs)

    matches = []
    unmatched_origin_indices = [] # Keep track of unmatched original sentence indices

    for comp_idx, row in enumerate(jaccard_matrix):
        top_score, best_origin_idx = row.max(0)
        if top_score >= JACCARD_THRESHOLD:
            row_origin = origin_records[best_origin_idx.item()]
            row_compared = compared_records[comp_idx]
            matches.append({
                "source_id": row_origin["source_id"],
                "original": row_origin["original"],
                "matched_file": row_compared["matched_file"],
                "matched_index": row_compared["matched_index"],
                "matched": row_compared["matched"],
                "similarity": top_score.item(),
                "order": row_origin["order"]
            })
        else:
            # If no match is found for this compared sentence, we don't know which
            # original sentence it should have matched with.
            # We need to identify the original sentences that did *not* have any match.
            pass

    # To find unmatched original sentences, we can iterate through each original sentence
    # and check if it was part of any match.
    matched_origin_indices = {m["order"][0] for m in matches}
    for i in range(len(origin_records)):
        if i not in matched_origin_indices:
            unmatched_origin_indices.append(i)

    print("\U0001f9ee é€²è¡ŒæœªåŒ¹é…å¥å­çš„è©å½™è£œæ•‘...")
    unmatched_records = [origin_records[i] for i in unmatched_origin_indices]

    extracted_keywords = extract_keywords(unmatched_records)
    keyword_matches = match_keywords_to_compared(extracted_keywords, compared_records)
    keyword_matches = remove_noisy_matches(keyword_matches)

    all_matches = matches + keyword_matches
    all_matches.sort(key=lambda x: (x["order"]))
    for m in all_matches:
        del m["order"]

    with open(OUTPUT_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(all_matches, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… å®Œæˆï¼å…± {len(all_matches)} ç­†çµæœï¼Œå·²å„²å­˜è‡³ {OUTPUT_JSON_PATH}")

if __name__ == "__main__":
    main()
