#!/usr/bin/env python3
"""
match_pipeline_simple.py (updated with progress bars and logging)

Usage:
  python match_pipeline_simple.py

é è¨­åƒæ•¸åœ¨æª”æ¡ˆé ‚ç«¯ï¼›æ­¤ç‰ˆæœ¬ï¼š
 - åªå° origin-text åšå‰å¾Œç¶´æ’é™¤
 - compared_text å®Œæ•´æ¸…ç†æ¨™é»å¾Œæ¯”å°ï¼Œä¸åšå‰å¾Œç¶´éæ¿¾
 - å°‡ origin-text èˆ‡ compared-text åˆ†åˆ¥åˆ†è©å¾Œï¼Œé€ä¸€æ¯”å°
"""
import json
import pickle
from pathlib import Path
from tqdm import tqdm
import Levenshtein
from ckip_transformers.nlp import CkipWordSegmenter
import logging
import re
from datetime import datetime

# è¨­å®š logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s: %(message)s",
)

# === User-adjustable defaults ===
TEXT_INPUT = Path("D:/lufu_allusion/data/raw/origin-text.txt")      # æª”æ¡ˆéœ€å« '---' åˆ†æ®µæ¨™è¨˜
COMPARED_DIRS_FILE = Path("D:/lufu_allusion/data/raw/compared_text")
OUTPUT_JSON = Path("D:/lufu_allusion/data/processed/results.json")
CACHE_FILE = Path("D:/lufu_allusion/cache/results.pkl")
DEVICE = 0
SIM_THRESHOLD = 45.0

# æ’é™¤å­—é¦–èˆ‡å­—å°¾ï¼ˆåªå° origin-textï¼‰
PREFIX_EXCLUDE = [
    "å¾’è§€å…¶","çŸ§å¤«","çŸ§ä¹ƒ","è‡³å¤«","æ‡¿å¤«","è“‹ç”±æˆ‘å›","é‡æ›°","æ˜¯çŸ¥","å¤«å…¶","æ‡¿å…¶","æ‰€ä»¥",
    "æƒ³å¤«","å…¶å§‹ä¹Ÿ","ç•¶å…¶","æ³å¾©","æ™‚å‰‡","è‡³è‹¥","è±ˆç¨","è‹¥ä¹ƒ","ä»Šå‰‡","ä¹ƒçŸ¥","æ—¢è€Œ","å—Ÿä¹",
    "æ•…æˆ‘å","è§€å¤«","ç„¶è€Œ","çˆ¾ä¹ƒ","æ˜¯ä»¥","åŸå¤«","æ›·è‹¥","æ–¯å‰‡","æ–¼æ™‚","æ–¹ä»Š","äº¦ä½•å¿…","è‹¥ç„¶",
    "å®¢æœ‰","è‡³æ–¼","å‰‡çŸ¥","ä¸”å¤«","æ–¯ä¹ƒ","æ³","æ–¼æ˜¯","è¦©å¤«","ä¸”å½¼","è±ˆè‹¥","å·²è€Œ","å§‹ä¹Ÿ","æ•…",
    "ç„¶å‰‡","è±ˆå¦‚æˆ‘","è±ˆä¸ä»¥","æˆ‘åœ‹å®¶","å…¶å·¥è€…","æ‰€è¬‚","ä»Šå¾å›","åŠå¤«","çˆ¾å…¶","å°‡ä»¥","å¯ä»¥",
    "ä»Š","åœ‹å®¶","ç„¶å¾Œ","å‘éæˆ‘å","å‰‡æœ‰","å½¼","æƒœä¹","ç”±æ˜¯","ä¹ƒè¨€æ›°","è‹¥å¤«","äº¦ä½•ç”¨","ä¸ç„¶",
    "å˜‰å…¶","ä»Šå‰‡","å¾’ç¾å¤«","æ•…èƒ½","æœ‰æ¢è€…æ›°","æƒœå¦‚","è€Œæ³","é€®å¤«","èª å¤«","æ–¼æˆ²","æ´ä¹","ä¼Šæ˜”",
    "å‰‡å°‡","ä»Šå‰‡","æ³ä»Š","å£«æœ‰","æš¨ä¹","äº¦ä½•è¾¨å¤«","ä¿¾å¤«","äº¦çŒ¶","ç»å¤«","æ™‚ä¹Ÿ","å›ºçŸ¥","è¶³ä»¥",
    "çŸ§åœ‹å®¶","æ¯”ä¹","äº¦ç”±","è§€å…¶","å°‡ä¿¾ä¹","è–äºº","å›å­","æ–¼ä»¥","ä¹ƒ","æ–¯è“‹","å™«","å¤«æƒŸ",
    "é«˜çš‡å¸","å¸æ—¢","å˜‰å…¶","å§‹å‰‡","åˆå®‰å¾—","å…¶","å„’æœ‰","ç•¶æ˜¯æ™‚ä¹Ÿ","å¤«ç„¶","å®œä¹","æ•…å…¶","åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ","ä»Šæˆ‘åœ‹å®¶","æ˜¯æ™‚","æœ‰å¸","å‘è‹¥","æˆ‘çš‡","æ•…ç‹è€…","å‰‡","é„’å­","å­°","æš¨å¤«","ç”¨èƒ½",
    "æ•…å°‡","æ³å…¶","æ•…å®œ","ç‹è€…","è–ä¸Š","å…ˆç‹","ä¹ƒæœ‰","æ³ä¹ƒ","åˆ¥æœ‰","ä»Šè€…","å›ºå®œ","çš‡ä¸Š","ä¸”å…¶",
    "å¾’è§€å¤«","å¸å ¯ä»¥","å§‹å…¶","å€è€Œ","ä¹ƒæ›°","å‘ä½¿","æ¼¢æ­¦å¸","å…ˆæ˜¯","ä»–æ—¥","ä¹ƒå‘½","è§€ä¹","åœ‹å®¶ä»¥",
    "å¢¨å­","å€Ÿå¦‚","è¶³ä»¥","ä¸Šä¹ƒ","å—šå‘¼","æ˜”ä¼Š","å…ˆè³¢","é‚ä½¿","è±ˆæ¯”å¤«","å›ºå…¶","æ³æœ‰","é­¯æ­ç‹","çš‡å®¶",
    "å¾å›æ˜¯æ™‚","çŸ¥","å‘¨ç©†ç‹","å‰‡æœ‰","æ˜¯ç”¨","ä¹ƒè¨€æ›°","åŠ","æ•…å¤«","çŸ§ä¹","å¤«ä»¥","å¯§ä»¤","å¦‚","ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ","é‚","æ‚²å¤«","å®‰å¾—","æ•…å¾—","ä¸”è¦‹å…¶","æ˜¯ä½•","è«ä¸","å£«æœ‰","çŸ¥å…¶","æœªè‹¥"
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰"]
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰/(),1234567890Â¶"

# === åŠŸèƒ½å‡½å¼ ===

def load_raw_text(path: Path) -> str:
    return path.read_text(encoding='utf-8')


def parse_documents(raw_text: str) -> list[dict]:
    docs = []
    for seg in raw_text.split('---'):
        seg = seg.strip()
        if not seg:
            continue
        lines = seg.splitlines()
        title = lines[0].split('ï¼š',1)[1].strip() if 'ï¼š' in lines[0] else lines[0]
        author = lines[1].split('ï¼š',1)[1].strip() if len(lines)>1 and 'ï¼š' in lines[1] else ''
        content = '\n'.join(lines[2:]).strip()
        docs.append({'title': title, 'author': author, 'content': content})
    return docs


def clean_text(text: str) -> str:
    return ''.join(ch for ch in text if ch not in CHARS_TO_REMOVE).strip()


def skip_prefix_suffix(text: str) -> bool:
    return any(text.startswith(p) for p in PREFIX_EXCLUDE) or any(text.endswith(s) for s in SUFFIX_EXCLUDE)


def load_compared_sentences(path: Path) -> tuple[list[str], list[tuple]]:
    paths = [path] if path.is_dir() else [Path(x.strip()) for x in path.read_text(encoding='utf-8').splitlines()]
    sents, meta = [], []
    for p in paths:
        for f in p.rglob('*.txt'):
            for idx, ln in enumerate(f.read_text(encoding='utf-8').splitlines()):
                txt = clean_text(ln)
                if not txt:
                    continue
                sents.append(txt)
                meta.append((p.name, f.stem, idx))
    return sents, meta

segmenter = None

def init_segmenter(device: int):
    global segmenter
    segmenter = CkipWordSegmenter(device=device)
    logging.info(f"âœ… CKIP initialized on device {device}")


def segment_batch(texts: list[str], batch_size: int = 1000) -> list[str]:
    out = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Segmenting", unit="æ‰¹æ¬¡"):
        batch = texts[i:i+batch_size]
        tok = segmenter(batch, show_progress=False)
        out.extend([" ".join(t) for t in tok])
    return out


def compute_matches(orig_tokens, comp_tokens, comp_meta, threshold):
    matches = []
    for i, o in enumerate(tqdm(orig_tokens, desc="æ¯”å° origin", unit="å¥")):
        for j, c in enumerate(tqdm(comp_tokens, desc="æ¯”å° compared", unit="å¥", leave=False)):
            sim = Levenshtein.ratio(o, c) * 100
            if sim >= threshold:
                matches.append({
                    'origin_index': i,
                    'comp_meta': comp_meta[j],
                    'similarity': sim,
                    'origin_token': o,
                    'comp_token': c
                })
    return matches


def load_cache(path: Path):
    return pickle.loads(path.read_bytes())


def save_cache(data, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_bytes(pickle.dumps(data))


def main():
    # 1. parse origin-text, apply prefix/suffix filter
    raw = load_raw_text(TEXT_INPUT)
    docs = parse_documents(raw)
    origin_sents = []
    for d in docs:
        for ln in d['content'].splitlines():
            txt = clean_text(ln)
            if txt and not skip_prefix_suffix(txt):
                origin_sents.append(txt)
    logging.info(f"âœ… {len(origin_sents)} origin sentences after filter")

    # 2. load and clean compared-text (no prefix/suffix skip)
    comp_sents, comp_meta = load_compared_sentences(COMPARED_DIRS_FILE)
    logging.info(f"âœ… {len(comp_sents)} compared sentences loaded")

    # 3. segment both
    init_segmenter(DEVICE)
    orig_tokens = segment_batch(origin_sents)
    comp_tokens = segment_batch(comp_sents)

    # 4. compute or load cache
    if CACHE_FILE.exists():
        matches = load_cache(CACHE_FILE)
        logging.info(f"âœ… Loaded {len(matches)} matches from cache")
    else:
        matches = compute_matches(orig_tokens, comp_tokens, comp_meta, SIM_THRESHOLD)
        save_cache(matches, CACHE_FILE)
        logging.info(f"âœ… Computed and cached {len(matches)} matches")

    # 5. write results
    OUTPUT_JSON.parent.mkdir(parents=True, exist_ok=True)
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump({'docs': docs, 'matches': matches}, f, ensure_ascii=False, indent=2)
    logging.info(f"âœ… Results written to {OUTPUT_JSON}")

if __name__ == '__main__':
    start = datetime.now()
    logging.info(f"ğŸ”„ ç¨‹å¼å•Ÿå‹•ï¼Œé–‹å§‹æ™‚é–“ï¼š{start}")
    try:
        main()
    except Exception:
        logging.exception("ğŸ’¥ ç¨‹å¼ä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤")
        raise
    finally:
        end = datetime.now()
        logging.info(f"âœ… ç¨‹å¼çµæŸï¼ŒçµæŸæ™‚é–“ï¼š{end}ï¼Œç¸½è€—æ™‚ï¼š{end - start}")
        print("\nğŸ‰ ğŸ‰ ğŸ‰  å…¨éƒ¨åŸ·è¡Œå®Œç•¢ï¼")
