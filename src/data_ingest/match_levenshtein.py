#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
data_ingest/match_levenshtein.py

å°æ¯” origin-text.txt è£¡æ¯ä¸€å¥èˆ‡ compared_text åº•ä¸‹æ‰€æœ‰ .txt ä¸­çš„å¥å­ï¼Œ
ç›¸ä¼¼åº¦ â‰¥ é–¾å€¼çš„çµæœè¼¸å‡ºç‚º JSON æ ¼å¼ï¼Œä¸¦å›å‚³çµæ§‹åŒ–è³‡æ–™ä¾›å¤–éƒ¨å‘¼å«ã€‚
"""
import json
import re
from pathlib import Path
from datetime import datetime
import logging
import Levenshtein
from tqdm import tqdm
from ckip_transformers.nlp import CkipWordSegmenter

# === Default parameters (å¯ç”±å‘¼å«ç«¯è¦†å¯«) ===
SIM_THRESHOLD   = 45.0  # ç›¸ä¼¼åº¦é–€æª»
DEVICE          = 0     # CKIP device id
# ç”¨æ–¼æ¯”è¼ƒæ™‚å°‡ compared_text åˆ‡å¥çš„åˆ†éš”ç¬¦
CHARS_TO_REMOVE = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰/(),1234567890Â¶"

# åœç”¨è©åˆ—è¡¨ï¼ˆåªå° origin-text æ‹†å¥å¾Œæ¯ç‰‡æ®µé€²è¡Œ stripï¼‰
PREFIX_EXCLUDE = [
    "å¾’è§€å…¶","çŸ§å¤«","çŸ§ä¹ƒ","è‡³å¤«","æ‡¿å¤«","è“‹ç”±æˆ‘å›","é‡æ›°","æ˜¯çŸ¥","å¤«å…¶","æ‡¿å…¶","æ‰€ä»¥",
    "æƒ³å¤«","å…¶å§‹ä¹Ÿ","ç•¶å…¶","æ³å¾©","æ™‚å‰‡","è‡³è‹¥","è±ˆç¨","è‹¥ä¹ƒ","ä»Šå‰‡","ä¹ƒçŸ¥","æ—¢è€Œ","å—Ÿä¹",
    "æ•…æˆ‘å","è§€å¤«","ç„¶è€Œ","çˆ¾ä¹ƒ","æ˜¯ä»¥","åŸå¤«","æ›·è‹¥","æ–¯å‰‡","æ–¼æ™‚","æ–¹ä»Š","äº¦ä½•å¿…","è‹¥ç„¶",
    "å®¢æœ‰","è‡³æ–¼","å‰‡çŸ¥","ä¸”å¤«","æ–¯ä¹ƒ","æ³","æ–¼æ˜¯","è¦©å¤«","ä¸”å½¼","è±ˆè‹¥","å·²è€Œ","å§‹ä¹Ÿ","æ•…",
    "ç„¶å‰‡","è±ˆå¦‚æˆ‘","è±ˆä¸ä»¥","æˆ‘åœ‹å®¶","å…¶å·¥è€…","æ‰€è¬‚","ä»Šå¾å›","åŠå¤«","çˆ¾å…¶","å°‡ä»¥","å¯ä»¥",
    "ä»Š","åœ‹å®¶","ç„¶å¾Œ","å‘éæˆ‘å","å‰‡æœ‰","å½¼","æƒœä¹","ç”±æ˜¯","ä¹ƒè¨€æ›°","è‹¥å¤«","äº¦ä½•ç”¨","ä¸ç„¶",
    "å˜‰å…¶","ä»Šå‰‡","å¾’ç¾å¤«","æ•…èƒ½","æœ‰æ¢è€…æ›°","æƒœå¦‚","è€Œæ³","é€®å¤«","èª å¤«","æ–¼æˆ²","æ´ä¹","ä¼Šæ˜”",
    "å‰‡å°‡","ä»Šå‰‡","æ³ä»Š","å£«æœ‰","æš¨ä¹","äº¦ä½•è¾¨å¤«","ä¿¾å¤«","äº¦çŒ¶","ç»å¤«","æ™‚ä¹Ÿ","å›ºçŸ¥","è¶³ä»¥",
    "çŸ§åœ‹å®¶","æ¯”ä¹","äº¦ç”±","è§€å…¶","å°‡ä¿¾ä¹","è–äºº","å›å­","æ–¼ä»¥","ä¹ƒ","æ–¯è“‹","å™«","å¤«æƒŸ",
    "é«˜çš‡å¸","å¸æ—¢","å˜‰å…¶","å§‹å‰‡","åˆå®‰å¾—","å…¶","å„’æœ‰","ç•¶æ˜¯æ™‚ä¹Ÿ","å¤«ç„¶","å®œä¹","æ•…å…¶","åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ","ä»Šæˆ‘åœ‹å®¶","æ˜¯æ™‚","æœ‰å¸","å‘è‹¥","æˆ‘çš‡","æ•…ç‹è€…","å‰‡","é„’å­","å­°","æš¨å¤«","ç”¨èƒ½",
    "æ•…å°‡","æ³å…¶","æ•…å®œ","ç‹è€…","è–ä¸Š","å…ˆç‹","ä¹ƒæœ‰","æ³ä¹ƒ","åˆ¥æœ‰","ä»Šè€…","å›ºå®œ","çš‡ä¸Š","ä¸”å…¶",
    "å¾’è§€å¤«","å¸å ¯ä»¥","å§‹å…¶","å€è€Œ","ä¹ƒæ›°","å‘ä½¿","æ¼¢æ­¦å¸","å…ˆæ˜¯","ä»–æ—¥","ä¹ƒå‘½","è§€ä¹","åœ‹å®¶ä»¥",
    "å¢¨å­","å€Ÿå¦‚","è¶³ä»¥","ä¸Šä¹ƒ","å—šå‘¼","æ˜”ä¼Š","å…ˆè³¢","é‚ä½¿","è±ˆæ¯”å¤«","å›ºå…¶","æ³æœ‰","é­¯æ­ç‹","çš‡å®¶",
    "å¾å›æ˜¯æ™‚","çŸ¥","å‘¨ç©†ç‹","å‰‡æœ‰","æ˜¯ç”¨","ä¹ƒè¨€æ›°","åŠ","æ•…å¤«","çŸ§ä¹","å¤«ä»¥","å¯§ä»¤","å¦‚","ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ","é‚","æ‚²å¤«","å®‰å¾—","æ•…å¾—","ä¸”è¦‹å…¶","æ˜¯ä½•","è«ä¸","å£«æœ‰","çŸ¥å…¶","æœªè‹¥"
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰"]

# -----------------------------------------------------------------------------
# æ—¥èªŒè¨­å®š
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s %(levelname)s: %(message)s")

# origin-text cleaning: åƒ…ç§»é™¤å‰å¾Œæ¨™ç±¤ï¼Œä¸å»é™¤æ¨™é»
def strip_prefix_suffix(text: str) -> str:
    original = text
    changed = True
    while changed:
        changed = False
        for p in PREFIX_EXCLUDE:
            # åƒ…å‰é™¤é•·åº¦è‡³å°‘2çš„å‰ç¶´ï¼Œä»¥å…èª¤åˆªå–®å­—
            if len(p) <= 1:
                continue
            if text.startswith(p):
                stripped = text[len(p):].lstrip()
                logging.debug(f"Stripped prefix '{p}' from '{original}' -> '{stripped}'")
                text = stripped
                changed = True
                break
    changed = True
    while changed:
        changed = False
        for s in SUFFIX_EXCLUDE:
            if len(s) <= 1:
                continue
            if text.endswith(s):
                stripped = text[:-len(s)].rstrip()
                logging.debug(f"Stripped suffix '{s}' from '{original}' -> '{stripped}'")
                text = stripped
                changed = True
                break
    return text

# è§£æ origin-text.txt ç‚ºæ¨™é¡Œã€ä½œè€…ã€å…§æ–‡åˆ—è¡¨ï¼Œä¸¦è·³é "è³¦ç¯‡ï¼š" / "è³¦å®¶ï¼š" æ¨™ç±¤è¡Œ ç‚ºæ¨™é¡Œã€ä½œè€…ã€å…§æ–‡åˆ—è¡¨ï¼Œä¸¦è·³é "è³¦ç¯‡ï¼š" / "è³¦å®¶ï¼š" æ¨™ç±¤è¡Œ
def load_raw_docs(path: Path) -> list[dict]:
    lines = path.read_text(encoding='utf-8').splitlines()
    logging.info(f"Reading origin document from {path} ({len(lines)} lines)")
    sections, buf = [], []
    for ln in lines:
        if ln.strip() == '---':
            if buf:
                sections.append(buf)
                buf = []
        else:
            buf.append(ln)
    if buf:
        sections.append(buf)
    logging.info(f"Split into {len(sections)} sections by '---'")

    docs = []
    for idx, sec in enumerate(sections, 1):
        title = sec[0].split('ï¼š',1)[1].strip() if 'ï¼š' in sec[0] else sec[0].strip()
        author = sec[1].split('ï¼š',1)[1].strip() if len(sec)>1 and 'ï¼š' in sec[1] else ''
        content_lines = []
        for ln in sec[2:]:
            if ln.startswith('è³¦ç¯‡ï¼š') or ln.startswith('è³¦å®¶ï¼š'):
                logging.debug(f"Skipped label in section {idx}: {ln}")
                continue
            content_lines.append(ln)
        content = '\n'.join(content_lines)
        docs.append({'title': title, 'author': author, 'content': content})
    logging.info(f"Loaded {len(docs)} docs from origin-text")
    return docs

# æ‹†å¥ä¸¦æ¸…æ´— origin å¥å­ï¼ˆä¿ç•™æ¨™é»ï¼Œç”¨æ–¼åˆ†è©ï¼‰
def split_and_clean_sentences(docs: list[dict]) -> list[str]:
    origin_sents = []
    for d in docs:
        parts = re.split(r'[ã€‚ï¼Œï¼ï¼Ÿï¼›]', d['content'])
        logging.debug(f"Doc '{d['title']}' split into {len(parts)} parts")
        for p in parts:
            txt = p.strip()
            if not txt:
                continue
            txt = strip_prefix_suffix(txt)
            if txt:
                origin_sents.append(txt)
    logging.info(f"Extracted {len(origin_sents)} origin sentences")
    return origin_sents

# è¼‰å…¥ä¸¦åˆ‡å¥ compared_text åº•ä¸‹æ‰€æœ‰ txtï¼Œåˆ†éš”ç¬¦ç‚º CHARS_TO_REMOVE
def load_compared_sentences(compared_dir: Path) -> tuple[list[str], list[tuple]]:
    files = list(compared_dir.rglob('*.txt'))
    logging.info(f"Found {len(files)} compared text files in {compared_dir}")
    sents, meta = [], []
    sep_pattern = f"[{re.escape(CHARS_TO_REMOVE)}]"
    logging.debug(f"Splitting compared text using pattern: {sep_pattern}")
    for f in files:
        raw = f.read_text(encoding='utf-8')
        parts = re.split(sep_pattern, raw)
        logging.debug(f"File {f.name} split into {len(parts)} parts by CHARS_TO_REMOVE")
        for idx, part in enumerate(parts):
            txt = part.strip()
            if not txt:
                continue
            sents.append(txt)
            meta.append((f.parent.name, f.stem, idx))
    logging.info(f"Loaded {len(sents)} compared sentences")
    return sents, meta

# CKIP åˆ†è©
_segmenter = None
def init_segmenter(device: int):
    global _segmenter
    _segmenter = CkipWordSegmenter(device=device)
    logging.info(f"CKIP initialized on device {device}")

def segment_batch(texts: list[str], batch_size: int=500) -> list[str]:
    out = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Segmenting", unit="batch"):
        toks = _segmenter(texts[i:i+batch_size], show_progress=False)
        out.extend([" ".join(t) for t in toks])
    logging.info(f"Segmented {len(out)} sentences into tokens")
    return out

# æ¯”å°ç›¸ä¼¼åº¦
def compute_matches(orig_tokens: list[str], comp_tokens: list[str], comp_meta: list[tuple], threshold: float) -> list[dict]:
    logging.info(f"Matching with threshold {threshold}%")
    matches = []
    for i, o in enumerate(tqdm(orig_tokens, desc="Matching origin", unit="sent")):
        for j, c in enumerate(comp_tokens):
            sim = Levenshtein.ratio(o, c) * 100
            if sim >= threshold:
                matches.append({
                    'origin_index': i,
                    'comp_meta': comp_meta[j],
                    'similarity': round(sim,1),
                    'origin_token': o,
                    'comp_token': c
                })
    logging.info(f"Found {len(matches)} total matches")
    return matches

# æ ¸å¿ƒå‡½å¼
def retrieve_direct_allusions(origin_path: Path, compared_dir: Path, device: int = DEVICE, threshold: float = SIM_THRESHOLD) -> dict:
    logging.info(f"Start retrieval: origin={origin_path}, compared={compared_dir}")
    docs = load_raw_docs(origin_path)
    origin_sents = split_and_clean_sentences(docs)
    comp_sents, comp_meta = load_compared_sentences(compared_dir)
    init_segmenter(device)
    orig_tokens = segment_batch(origin_sents)
    comp_tokens = segment_batch(comp_sents)
    matches = compute_matches(orig_tokens, comp_tokens, comp_meta, threshold)
    return {'docs': docs, 'matches': matches}

# CLI
if __name__ == '__main__':
    start = datetime.now()
    logging.info(f"Program start: {start}")
    try:
        # è¨­å®šæª”æ¡ˆè·¯å¾‘
        origin = Path(r"C:\Users\TAOYI CHIANG\OneDrive\æ¡Œé¢\origin-text-test.txt")
        compared_dir = Path(r"C:\Users\TAOYI CHIANG\OneDrive\æ¡Œé¢\compared_text")
        output_json = Path(r"D:\lufu_allusion\data\processed\results.json")

        # åŸ·è¡Œæª¢ç´¢
        result = retrieve_direct_allusions(origin, compared_dir)

        # å¯«å…¥ JSON
        output_json.parent.mkdir(parents=True, exist_ok=True)
        output_json.write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding='utf-8')
        logging.info(f"Results saved to {output_json}")
    except Exception as e:
        logging.exception("Unexpected error during CLI execution")
        raise
    finally:
        end = datetime.now()
        logging.info(f"Program end: {end} (elapsed {end - start})")
print("\nğŸ‰ Done!")
