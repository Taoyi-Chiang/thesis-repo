# é—œé–‰ Huggingface Transformers å…§éƒ¨é€²åº¦æ¢
import os
os.environ['TRANSFORMERS_NO_TQDM'] = 'true'
from transformers import logging
logging.disable_progress_bar()

# ä½¿ç”¨å‰è«‹å…ˆå®‰è£å¥—ä»¶ï¼š
# pip install ckip-transformers transformers torch
import json
from pathlib import Path
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from ckip_transformers.nlp import CkipWordSegmenter
from contextlib import redirect_stdout, redirect_stderr

# ========== ä½¿ç”¨è€…è¨­å®š (ä¸€è™•å¯æ§) ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")
OUTPUT_VOCAB_FOLDER = Path(r"D:/lufu_allusion/data/processed/vocabularies")
# BERT-based language model for segmentation
BERT_LM = "bert-base"
# CKIP Transformers WS ä»»å‹™æ¨¡å‹
WS_MODEL = "ckiplab/bert-base-chinese-ws"
# GPT2 for åˆ†è©æ ¡æ­£
GPT2_MODEL = "ckiplab/gpt2-base-chinese"
# åˆ†è©çµæœè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
OUT_PATH = OUTPUT_VOCAB_FOLDER / "parsed_results_tokens_ckip_corrected.json"

# åˆå§‹è¼‰å…¥åˆ†è©å™¨å’Œ GPT2-LM åªè·‘ä¸€æ¬¡
tokenizer_segmenter = CkipWordSegmenter(
    model=BERT_LM,
    model_name=WS_MODEL,
    device=0
)
# GPT2 tokenizer èˆ‡ LM
gpt2_tok = AutoTokenizer.from_pretrained(GPT2_MODEL)
gpt2_lm = AutoModelForCausalLM.from_pretrained(GPT2_MODEL).to("cuda:0")

def clean_sentence(text):
    """
    ç§»é™¤å¥å­é¦–å°¾ç©ºç™½ã€‚
    """
    return text.strip()


def score_sentence(text):
    """
    ç”¨ GPT2 LM è¨ˆç®—å¹³å‡è² å°æ•¸ä¼¼ç„¶ (loss)ï¼Œå€¼è¶Šä½ä»£è¡¨è¶Šè‡ªç„¶ã€‚
    """
    inputs = gpt2_tok(text, return_tensors="pt").to("cuda:0")
    with torch.no_grad():
        outputs = gpt2_lm(**inputs, labels=inputs["input_ids"])
    return outputs.loss.item()


def correct_segmentation(words, max_iter=3):
    """
    åœ¨åˆæ¬¡æ–·è©çµæœä¸Šï¼Œé€²è¡Œå¤šç¨®æ ¡æ­£ç­–ç•¥ï¼š
    1. åˆä½µç›¸é„°è©ï¼ˆmergeï¼‰
    2. æ‹†åˆ†éé•·è©ï¼ˆsplitï¼Œåªå°é•·åº¦>=3çš„è©ï¼‰
    3. é‚Šç•Œç§»å‹•ï¼ˆmove_boundaryï¼‰
    åè¦†åŸ·è¡Œç›´åˆ°ç„¡æ³•æ”¹é€²æˆ–é”åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•¸ã€‚
    """
    if not words:
        return []
    best = words
    best_score = score_sentence("".join(best))
    for _ in range(max_iter):
        improved = False
        # 1. Merge: åˆä½µç›¸é„°è©
        for i in range(len(best) - 1):
            merged = best[:i] + [best[i] + best[i+1]] + best[i+2:]
            sc = score_sentence("".join(merged))
            if sc < best_score:
                best_score, best = sc, merged
                improved = True
        # 2. Split: åªå°é•·åº¦>=3çš„è©å˜—è©¦æ‹†åˆ†
        for i, w in enumerate(best):
            if len(w) >= 3:
                for pos in range(1, len(w)):
                    split = best[:i] + [w[:pos], w[pos:]] + best[i+1:]
                    sc = score_sentence("".join(split))
                    if sc < best_score:
                        best_score, best = sc, split
                        improved = True
        # 3. Move boundary: èª¿æ•´ç›¸é„°é‚Šç•Œ
        for i in range(len(best) - 1):
            left, right = best[i], best[i+1]
            # å¾ left ç§»ä¸€å­—åˆ° right
            if len(left) > 1:
                move_lr = best[:i] + [left[:-1], left[-1] + right] + best[i+2:]
                sc = score_sentence("".join(move_lr))
                if sc < best_score:
                    best_score, best = sc, move_lr
                    improved = True
            # å¾ right ç§»ä¸€å­—åˆ° left
            if len(right) > 1:
                move_rl = best[:i] + [left + right[0], right[1:]] + best[i+2:]
                sc = score_sentence("".join(move_rl))
                if sc < best_score:
                    best_score, best = sc, move_rl
                    improved = True
        if not improved:
            break
    return best


def flatten_sentences(parsed_data):
    """
    å°‡å¤šå±¤çµæ§‹å±•å¹³ï¼Œè¿”å›å¥å­åˆ—è¡¨èˆ‡æ˜ å°„ç´¢å¼•ã€‚
    """
    texts, index_map = [], []
    for ai, article in enumerate(parsed_data):
        for pi, para in enumerate(article.get("æ®µè½", [])):
            for gi, group in enumerate(para.get("å¥çµ„", [])):
                for si, sent in enumerate(group.get("å¥å­", [])):
                    content = sent.get("å…§å®¹", "")
                    txt = clean_sentence(content)
                    if txt:
                        texts.append(txt)
                        index_map.append((ai, pi, gi, si))
    return texts, index_map


def rebuild_results(parsed_data, tokens_list, index_map):
    """
    æ ¹æ“šæ˜ å°„å°‡ tokens_list æ”¾å›å°æ‡‰çµæ§‹ã€‚
    """
    result = []
    for article in parsed_data:
        art = {"ç¯‡è™Ÿ": article.get("ç¯‡è™Ÿ"), "è³¦å®¶": article.get("è³¦å®¶", ""),
               "è³¦ç¯‡": article.get("è³¦ç¯‡", ""), "æ®µè½": []}
        for para in article.get("æ®µè½", []):
            p = {"æ®µè½ç·¨è™Ÿ": para.get("æ®µè½ç·¨è™Ÿ"), "å¥çµ„": []}
            for group in para.get("å¥çµ„", []):
                p["å¥çµ„"].append({"å¥çµ„ç·¨è™Ÿ": group.get("å¥çµ„ç·¨è™Ÿ"), "å¥å­": []})
            art["æ®µè½"].append(p)
        result.append(art)
    for tokens, (ai, pi, gi, si) in zip(tokens_list, index_map):
        sent_obj = parsed_data[ai]["æ®µè½"][pi]["å¥çµ„"][gi]["å¥å­"][si]
        item = {"å¥ç·¨è™Ÿ": sent_obj.get("å¥ç·¨è™Ÿ"), "åŸå§‹": sent_obj.get("å…§å®¹", ""),
                "cleaned": clean_sentence(sent_obj.get("å…§å®¹", "")), "tokens": tokens}
        result[ai]["æ®µè½"][pi]["å¥çµ„"][gi]["å¥å­"].append(item)
    return result


def main():
    # 1. è®€å–è³‡æ–™
    print("ğŸ” é–‹å§‹è®€å– JSON æª”æ¡ˆ...")
    try:
        with PARSED_RESULTS_PATH.open("r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸè®€å– JSONï¼Œå…±è¼‰å…¥ {len(data)} ç¯‡æ–‡ç« ã€‚")
    except Exception as e:
        print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return

    # 2. å±•å¹³å¥å­
    texts, index_map = flatten_sentences(data)
    total = len(texts)
    print(f"ğŸªš å±•å¹³çµæ§‹å®Œæˆï¼Œç¸½å…± {total} å¥å¾…æ–·è©ã€‚")
    if total == 0:
        print("âš ï¸ ç„¡å¥å­å¯è™•ç†ï¼Œè«‹æª¢æŸ¥ JSON çµæ§‹ã€‚")
        return

    # 3. é€å¥åˆ†è© + æ ¡æ­£
    print("ğŸ“ é–‹å§‹åˆ†è©ä¸¦ä½¿ç”¨ GPT2 æ ¡æ­£...")
    tokens_list = []
    null_path = os.devnull
    for txt in tqdm(texts, desc="åˆ†è©æ ¡æ­£é€²åº¦"):
        with open(null_path, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull):
            try:
                init = tokenizer_segmenter([txt])[0]
            except:
                init = []
        corrected = correct_segmentation(init)
        tokens_list.append(corrected)
    print(f"âœ… åˆ†è©æ ¡æ­£å®Œæˆï¼Œå…±è™•ç† {len(tokens_list)} å¥ã€‚")

    # 4. é‡å»ºçµæ§‹ä¸¦å„²å­˜çµæœ
    print("ğŸ”„ é‡å»ºåŸå§‹çµæ§‹...")
    parsed_tokens = rebuild_results(data, tokens_list, index_map)
    OUTPUT_VOCAB_FOLDER.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ å„²å­˜çµæœè‡³ {OUT_PATH}...")
    try:
        with OUT_PATH.open("w", encoding="utf-8") as f:
            json.dump(parsed_tokens, f, ensure_ascii=False, indent=2)
        print("âœ… çµæœå„²å­˜å®Œæˆã€‚")
    except Exception as e:
        print(f"âŒ å„²å­˜å¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()
