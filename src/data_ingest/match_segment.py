# é—œé–‰ Huggingface Transformers å…§éƒ¨é€²åº¦æ¢
import os
os.environ['TRANSFORMERS_NO_TQDM'] = 'true'
from transformers import logging
logging.disable_progress_bar()

# ä½¿ç”¨å‰è«‹å…ˆå®‰è£ CKIP Transformersï¼š
# pip install ckip-transformers
import json
from pathlib import Path
from tqdm import tqdm
from ckip_transformers.nlp import CkipWordSegmenter
from contextlib import redirect_stdout, redirect_stderr

# ========== ä½¿ç”¨è€…è¨­å®š (ä¸€è™•å¯æ§) ==========
PARSED_RESULTS_PATH = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")
OUTPUT_VOCAB_FOLDER = Path(r"D:/lufu_allusion/data/processed/vocabularies")
# BERT-based language model and word segmentation model
BERT_LM = "bert-base"
WS_MODEL = "ckiplab/bert-base-chinese-ws"
# åˆ†è©çµæœè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
OUT_PATH = OUTPUT_VOCAB_FOLDER / "parsed_results_tokens_ckip.json"


def clean_sentence(text):
    """
    ç§»é™¤å¥å­é¦–å°¾ç©ºç™½ã€‚
    """
    return text.strip()


def flatten_sentences(parsed_data):
    """
    å°‡å¤šå±¤çµæ§‹å±•å¹³ï¼Œè¿”å›å¥å­åˆ—è¡¨èˆ‡æ˜ å°„ç´¢å¼•ã€‚
    """
    texts = []
    index_map = []  # (art_i, para_i, group_i, sent_i)
    for ai, article in enumerate(parsed_data):
        for pi, para in enumerate(article.get("æ®µè½", [])):
            for gi, group in enumerate(para.get("å¥çµ„", [])):
                for si, sent in enumerate(group.get("å¥å­", [])):
                    content = sent.get("å…§å®¹", "")
                    txt = clean_sentence(content)
                    if txt:
                        texts.append(txt)
                        index_map.append((ai, pi, gi, si))
    return texts, index_map


def rebuild_results(parsed_data, tokens_list, index_map):
    """
    æ ¹æ“šæ˜ å°„å°‡ tokens_list æ”¾å›å°æ‡‰çµæ§‹ã€‚
    """
    result = []
    # å»ºç«‹ç©ºæ¡†æ¶
    for article in parsed_data:
        result.append({
            "ç¯‡è™Ÿ": article.get("ç¯‡è™Ÿ"),
            "è³¦å®¶": article.get("è³¦å®¶", ""),
            "è³¦ç¯‡": article.get("è³¦ç¯‡", ""),
            "æ®µè½": []
        })
        for para in article.get("æ®µè½", []):
            result[-1]["æ®µè½"].append({
                "æ®µè½ç·¨è™Ÿ": para.get("æ®µè½ç·¨è™Ÿ"),
                "å¥çµ„": []
            })
            for group in para.get("å¥çµ„", []):
                result[-1]["æ®µè½"][-1]["å¥çµ„"].append({
                    "å¥çµ„ç·¨è™Ÿ": group.get("å¥çµ„ç·¨è™Ÿ"),
                    "å¥å­": []
                })
    # å¡«å…¥ tokens
    for tokens, (ai, pi, gi, si) in zip(tokens_list, index_map):
        sent_obj = parsed_data[ai]["æ®µè½"][pi]["å¥çµ„"][gi]["å¥å­"][si]
        result[ai]["æ®µè½"][pi]["å¥çµ„"][gi]["å¥å­"].append({
            "å¥ç·¨è™Ÿ": sent_obj.get("å¥ç·¨è™Ÿ"),
            "åŸå§‹": sent_obj.get("å…§å®¹", ""),
            "cleaned": clean_sentence(sent_obj.get("å…§å®¹", "")),
            "tokens": tokens
        })
    return result


def main():
    # 1. è®€å–è³‡æ–™
    print("ğŸ” é–‹å§‹è®€å– JSON æª”æ¡ˆ...")
    try:
        with PARSED_RESULTS_PATH.open("r", encoding="utf-8") as f:
            data = json.load(f)
        print(f"âœ… æˆåŠŸè®€å– JSONï¼Œå…±è¼‰å…¥ {len(data)} ç¯‡æ–‡ç« ã€‚")
    except Exception as e:
        print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•—ï¼š{e}")
        return

    # 2. åˆå§‹åŒ– CKIP åˆ†è©å™¨ (ä½¿ç”¨ GPU)
    print(f"ğŸ”§ åˆå§‹åŒ– CKIP åˆ†è©å™¨ (LM={BERT_LM}, WS={WS_MODEL}, device=0)...")
    try:
        segmenter = CkipWordSegmenter(
            model=BERT_LM,
            model_name=WS_MODEL,
            device=0
        )
        print("âœ… CKIP åˆ†è©å™¨è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨ GPUã€‚")
    except Exception as e:
        print(f"âŒ è¼‰å…¥ CKIP åˆ†è©å™¨æ™‚å‡ºéŒ¯ï¼š{e}")
        return

    # 3. å±•å¹³å¥å­ï¼Œæª¢æŸ¥å¥å­æ•¸
    texts, index_map = flatten_sentences(data)
    total = len(texts)
    print(f"ğŸªš å±•å¹³çµæ§‹å®Œæˆï¼Œç¸½å…± {total} å¥å¾…æ–·è©ã€‚")
    if total == 0:
        print("âš ï¸ æ²’æœ‰å¥å­å¯ä¾›è™•ç†ï¼Œè«‹æª¢æŸ¥ JSON çµæ§‹ã€‚")
        return

    # 4. é€å¥åˆ†è©ï¼Œåªé¡¯ç¤ºä¸€æ¢é€²åº¦ï¼Œä¸¦æŠ‘åˆ¶å…§éƒ¨è¼¸å‡º
    print("ğŸ“ é–‹å§‹é€å¥åˆ†è©...")
    tokens_list = []
    null_path = os.devnull
    for txt in tqdm(texts, desc="åˆ†è©é€²åº¦"):
        with open(null_path, 'w') as fnull, redirect_stdout(fnull), redirect_stderr(fnull):
            try:
                toks = segmenter([txt])[0]
            except Exception:
                toks = []
        tokens_list.append(toks)
    print(f"âœ… åˆ†è©å®Œæˆï¼Œå…±è™•ç† {len(tokens_list)} å¥ã€‚")

    # 5. é‡å»ºçµæ§‹ä¸¦å„²å­˜çµæœ
    parsed_tokens = rebuild_results(data, tokens_list, index_map)
    OUTPUT_VOCAB_FOLDER.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ’¾ å„²å­˜åˆ†è©çµæœè‡³ {OUT_PATH}...")
    try:
        with OUT_PATH.open("w", encoding="utf-8") as f:
            json.dump(parsed_tokens, f, ensure_ascii=False, indent=2)
        print("âœ… åˆ†è©çµæœå„²å­˜å®Œæˆã€‚")
    except Exception as e:
        print(f"âŒ å„²å­˜åˆ†è©çµæœå¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()
