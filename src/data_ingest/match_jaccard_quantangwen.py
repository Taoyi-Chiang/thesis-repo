import json
import re
import unicodedata
from pathlib import Path
from tqdm import tqdm
from bs4 import BeautifulSoup
from ckip_transformers.nlp import CkipWordSegmenter
import cupy as cp  # GPU acceleration with CuPy

# ========== ä½¿ç”¨è€…è¨­å®š ==========
PARSED_RESULTS_PATH   = Path(r"D:/lufu_allusion/data/processed/parsed_results.json")
COMPARED_FOLDER_PATH  = Path(r"D:/lufu_allusion/data/raw/compared_text/")
QUANTANG_HTML_PATH    = Path(r"D:/lufu_allusion/data/raw/quantangwen.html")  # å–®ä¸€ HTML æª”æ¡ˆ
OUTPUT_JSON_PATH      = Path(r"D:/lufu_allusion/data/processed/ALL_match_results_jaccard.json")
CHARS_TO_REMOVE       = "ã€‚ï¼Œã€ï¼šï¼›ï¼ï¼Ÿï¼ˆï¼‰ã€”ã€•ã€Œã€[]ã€ã€ã€Šã€‹ã€ˆã€‰\\#\\-\\ï¼\\(\\)\\[\\]\\\\/ ,.:;!?~1234567890Â¶"
JACCARD_THRESHOLD     = 0.7
BATCH_SIZE            = 16384
NUM_FEATURES          = 2 ** 17  # Hashing Trick ç¶­åº¦ï¼Œå¯æ ¹æ“š GPU è¨˜æ†¶é«”èª¿æ•´
MIN_TOKEN_LEN         = 2       # éçŸ­å¥å­éæ¿¾ä¸‹é™

# ========== åœç”¨è© & æ¸…æ´—å·¥å…· ==========
PREFIX_EXCLUDE = [
    "å¾’è§€å…¶", "çŸå¤«", "çŸä¹ƒ", "è‡³å¤«", "æ‡¿å¤«", "è“‹ç”±æˆ‘å›", "é‡æ›°", "æ˜¯çŸ¥", "å—Ÿå¤«", "å¤«å…¶", "æ‡¿å…¶", "æ‰€ä»¥",
    "æƒ³å¤«", "å…¶å§‹ä¹Ÿ", "ç•¶å…¶", "æ³å¾©", "æ™‚å‰‡", "è‡³è‹¥", "è±ˆç¨", "è‹¥ä¹ƒ", "ä»Šå‰‡", "ä¹ƒçŸ¥", "æ—¢è€Œ", "å—Ÿä¹",
    "æ•…æˆ‘å", "è§€å¤«", "ç„¶è€Œ", "çˆ¾ä¹ƒ", "æ˜¯ä»¥", "åŸå¤«", "æ›·è‹¥", "æ–¯å‰‡", "æ–¼æ™‚", "æ–¹ä»Š", "äº¦ä½•å¿…", "è‹¥ç„¶",
    "å®¢æœ‰", "è‡³æ–¼", "å‰‡çŸ¥", "ä¸”å¤«", "æ–¯ä¹ƒ", "æ³", "æ–¼æ˜¯", "è¦©å¤«", "ä¸”å½¼", "è±ˆè‹¥", "å·²è€Œ", "å§‹ä¹Ÿ", "æ•…",
    "ç„¶å‰‡", "è±ˆå¦‚æˆ‘", "è±ˆä¸ä»¥", "æˆ‘åœ‹å®¶", "å…¶å·¥è€…", "æ‰€è¬‚", "ä»Šå¾å›", "åŠå¤«", "çˆ¾å…¶", "å°‡ä»¥", "å¯ä»¥", "ä»Š",
    "åœ‹å®¶", "ç„¶å¾Œ", "å‘éæˆ‘å", "å‰‡æœ‰", "å½¼", "æƒœä¹", "ç”±æ˜¯", "ä¹ƒè¨€æ›°", "è‹¥å¤«", "äº¦ä½•ç”¨", "ä¸ç„¶",
    "å˜‰å…¶", "ä»Šå‰‡", "å¾’ç¾å¤«", "æ•…èƒ½", "æœ‰æ¢è€…æ›°", "æƒœå¦‚", "è€Œæ³", "é€®å¤«", "èª å¤«", "æ–¼æˆ²", "æ´ä¹", "ä¼Šæ˜”",
    "å‰‡å°‡", "ä»Šå‰‡", "æ³ä»Š", "å£«æœ‰", "æš¨ä¹", "äº¦ä½•è¾¨å¤«", "ä¿¾å¤«", "äº¦çŒ¶", "ç»å¤«", "æ™‚ä¹Ÿ", "å›ºçŸ¥", "è¶³ä»¥",
    "çŸåœ‹å®¶", "æ¯”ä¹", "äº¦ç”±", "è§€å…¶", "å°‡ä¿¾ä¹", "è–äºº", "å›å­", "æ–¼ä»¥", "ä¹ƒ", "æ–¯è“‹", "å™«", "å¤«æƒŸ",
    "é«˜çš‡å¸", "å¸æ—¢", "å˜‰å…¶", "å§‹å‰‡", "åˆå®‰å¾—", "å…¶", "å„’æœ‰", "ç•¶æ˜¯æ™‚ä¹Ÿ", "å¤«ç„¶", "å®œä¹", "æ•…å…¶", "åœ‹å®¶",
    "çˆ¾å…¶å§‹ä¹Ÿ", "ä»Šæˆ‘åœ‹å®¶", "æ˜¯æ™‚", "æœ‰å¸", "å‘è‹¥", "æˆ‘çš‡", "æ•…ç‹è€…", "å‰‡", "é„’å­", "å­°", "æš¨å¤«", "ç”¨èƒ½",
    "æ•…å°‡", "æ³å…¶", "æ•…å®œ", "ç‹è€…", "è–ä¸Š", "å…ˆç‹", "ä¹ƒæœ‰", "æ³ä¹ƒ", "åˆ¥æœ‰", "ä»Šè€…", "å›ºå®œ", "çš‡ä¸Š", "ä¸”å…¶",
    "å¾’è§€å¤«", "å¸å ¯ä»¥", "å§‹å…¶", "å€è€Œ", "ä¹ƒæ›°", "å‘ä½¿", "æ¼¢æ­¦å¸", "å…ˆæ˜¯", "ä»–æ—¥", "ä¹ƒå‘½", "è§€ä¹", "åœ‹å®¶ä»¥",
    "å¢¨å­", "å€Ÿå¦‚", "è¶³ä»¥", "ä¸Šä¹ƒ", "å—šå‘¼", "æ˜”ä¼Š", "å…ˆè³¢", "é‚ä½¿", "è±ˆæ¯”å¤«", "å›ºå…¶", "æ³æœ‰", "é­¯æ­ç‹", "çš‡å®¶",
    "å¾å›æ˜¯æ™‚", "çŸ¥", "å‘¨ç©†ç‹", "å‰‡æœ‰", "æ˜¯ç”¨", "ä¹ƒè¨€æ›°", "åŠ", "æ•…å¤«", "çŸä¹", "å¤«ä»¥", "å¯§ä»¤", "å¦‚", "ç„¶å‰‡",
    "æ»…æ˜ä¹ƒ", "é‚", "æ‚²å¤«", "å®‰å¾—", "æ•…å¾—", "ä¸”è¦‹å…¶", "æ˜¯ä½•", "è«ä¸", "å£«æœ‰", "çŸ¥å…¶", "æœªè‹¥"
]
SUFFIX_EXCLUDE = ["æ›°", "å“‰", "çŸ£", "ä¹Ÿ", "çŸ£å“‰", "ä¹", "ç„‰"]

def clean_sentence(text):
    for prefix in PREFIX_EXCLUDE:
        if text.startswith(prefix):
            return text[len(prefix):].strip()
    for suffix in SUFFIX_EXCLUDE:
        if text.endswith(suffix):
            return text[:-len(suffix)].strip()
    return text.strip()

def normalize(text: str) -> str:
    return unicodedata.normalize("NFKC", text)

# ========== HTML è½‰ç´”æ–‡å­— ==========
def html_to_text(html: str) -> str:
    soup = BeautifulSoup(html, 'html.parser')
    for br in soup.find_all('br'):
        br.replace_with("\n")
    for tag in soup.find_all(['p','div','li','h1','h2','h3','h4']):
        tag.append("\n")
    text = soup.get_text()
    lines = []
    for line in text.splitlines():
        if line.strip():
            lines.append(line.rstrip())
        else:
            if lines and lines[-1] != "":
                lines.append("")
    return "\n".join(lines)

# ========== è®€å– & æ¸…æ´— ==========
def load_parsed_results(path):
    """
    è¼‰å…¥å·²è§£æçš„ JSON æ ¼å¼åŸå§‹æ–‡æœ¬è³‡æ–™ã€‚

    Args:
        path (Path): åŒ…å«åŸå§‹æ–‡æœ¬è³‡æ–™çš„ JSON æª”æ¡ˆè·¯å¾‘ã€‚

    Returns:
        list: åŒ…å«åŸå§‹æ–‡æœ¬è¨˜éŒ„çš„åˆ—è¡¨ï¼Œæ¯æ¢è¨˜éŒ„ç‚ºä¸€å€‹å­—å…¸ã€‚
    """
    print("ğŸ”„ è¼‰å…¥åŸå¥è³‡æ–™â€¦")
    data = json.loads(path.read_text(encoding="utf-8"))
    records = []
    for art in data:
        for para in art.get("æ®µè½", []):
            for grp in para.get("å¥çµ„", []):
                for sent in grp.get("å¥å­", []):
                    orig = normalize(sent.get("å…§å®¹", ""))
                    fm = clean_sentence(orig)
                    if fm:
                        records.append({
                            "original": orig,
                            "for_matching": fm,
                            "article_num": art.get("ç¯‡è™Ÿ"),
                            "author": art.get("è³¦å®¶"),
                            "article_title": art.get("è³¦ç¯‡"),
                            "paragraph_num": para.get("æ®µè½ç·¨è™Ÿ"),
                            "group_num": grp.get("å¥çµ„ç·¨è™Ÿ"),
                            "sentence_num": sent.get("å¥ç·¨è™Ÿ")
                        })
    print(f"âœ… åŸå¥è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå…± {len(records)} æ¢ã€‚")
    return records



def load_compared_sentences(folder, chars):
    """
    è¼‰å…¥å¾…æ¯”å°çš„æ–‡æœ¬è³‡æ–™ï¼Œä¾†è‡ªæŒ‡å®šè³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰ .txt æª”æ¡ˆã€‚

    Args:
        folder (Path): åŒ…å«å¾…æ¯”å°æ–‡æœ¬æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ã€‚
        chars (str): è¦ç”¨æ–¼åˆ‡åˆ†å¥å­çš„å­—å…ƒé›†ã€‚

    Returns:
        list: åŒ…å«å¾…æ¯”å°å¥å­è¨˜éŒ„çš„åˆ—è¡¨ï¼Œæ¯æ¢è¨˜éŒ„ç‚ºä¸€å€‹å­—å…¸ã€‚
    """
    print("ğŸ”„ è¼‰å…¥å¾…æ¯”å°å¥è³‡æ–™â€¦")
    pattern = f"[{re.escape(chars)}]"
    sents = []
    for fp in folder.rglob("*.txt"):
        raw = normalize(fp.read_text(encoding="utf-8").replace("\n", ""))
        raw = re.sub(r"<[^>]*>", "", raw)  # ç§»é™¤ HTML æ¨™ç±¤
        for idx, seg in enumerate(re.split(pattern, raw)):
            seg = normalize(seg)
            fm = clean_sentence(seg)
            if fm:
                sents.append({
                    "raw": seg,
                    "for_matching": fm,
                    "matched_file": fp.parent.name + "/" + fp.stem,
                    "matched_index": idx
                })
    print(f"âœ… å¾…æ¯”å°è³‡æ–™è¼‰å…¥å®Œæˆï¼Œå…± {len(sents)} æ¢ã€‚")
    return sents



def load_quantang_sentences(folder, chars):
    """
    è¼‰å…¥å…¨å”æ–‡çš„æ–‡æœ¬è³‡æ–™ï¼Œä¾†è‡ªæŒ‡å®šè³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰ .html æª”æ¡ˆã€‚

    Args:
        folder (Path): åŒ…å«å…¨å”æ–‡ HTML æª”æ¡ˆçš„è³‡æ–™å¤¾è·¯å¾‘ã€‚
        chars (str): è¦ç”¨æ–¼åˆ‡åˆ†å¥å­çš„å­—å…ƒé›†ã€‚

    Returns:
        list: åŒ…å«å…¨å”æ–‡å¥å­è¨˜éŒ„çš„åˆ—è¡¨ï¼Œæ¯æ¢è¨˜éŒ„ç‚ºä¸€å€‹å­—å…¸ã€‚
    """
    print("ğŸ”„ è¼‰å…¥å…¨å”æ–‡èªæ–™â€¦")
    pattern = f"[{re.escape(chars)}]"
    sents = []
    for fp in folder.rglob("*.html"):
        try:
            html = fp.read_text(encoding="utf-8")
            text = html_to_text(html)  # ä½¿ç”¨ä¿®æ­£å¾Œçš„ html_to_text
            # å…ˆæŒ‰è¡Œåˆ†æ®µï¼Œå†ä¾ CHARS_TO_REMOVE åˆ‡å¥
            for line in text.splitlines():
                line = normalize(line.strip())
                if not line:
                    continue
                for seg in re.split(pattern, line):
                    fm = clean_sentence(seg)
                    if fm:
                        sents.append({"for_matching": fm})
        except Exception as e:
            print(f"Error processing file: {fp}, error: {e}")  # æ‰“å°éŒ¯èª¤è¨Šæ¯
    print(f"âœ… å…¨å”æ–‡è¼‰å…¥å®Œæˆï¼Œå…± {len(sents)} æ¢å¥å­ã€‚")
    return sents

# ========== åˆ†è© ==========
def segment_in_batches(records, segmenter, batch_size=2048, text_type=""):
    tokens = []
    with tqdm(total=len(records), desc=f"åˆ†è© ({text_type})", unit="å¥") as pbar:
        for i in range(0, len(records), batch_size):
            batch = [r['for_matching'] for r in records[i:i+batch_size]]
            toks = segmenter(batch, show_progress=False)
            tokens.extend(toks)
            pbar.update(len(batch))
    return tokens

# ========== Hashing Trick å‘é‡åŒ– ==========
def tokens_to_gpu_hash_matrix(tokens_list, num_features=NUM_FEATURES):
    mat = cp.zeros((len(tokens_list), num_features), dtype=cp.int8)
    for i, toks in enumerate(tokens_list):
        for w in toks:
            mat[i, (hash(w) & 0x7FFFFFFF) % num_features] = 1
    return mat

# ========== ç²¾ç¢º Jaccard ==========
def exact_jaccard(a, b):
    inter = len(a & b)
    union = len(a | b)
    return inter / union if union else 0.0

# ========== æ‰¹æ¬¡ Jaccard (GPU) ==========
def batch_jaccard_gpu(comp_mat, origin_mat):
    cf = comp_mat.astype(cp.float16)
    of = origin_mat.astype(cp.float16)
    inter = cf.dot(of.T)
    sc = cf.sum(axis=1, keepdims=True)
    so = of.sum(axis=1, keepdims=True).T
    jac = inter / (sc + so - inter + 1e-9)
    return jac.max(axis=1), jac.argmax(axis=1)

# ========== ä¸»ç¨‹å¼ ==========
def main():
    # 1. è¼‰å…¥
    origin = load_parsed_results(PARSED_RESULTS_PATH)
    compared = load_compared_sentences(COMPARED_FOLDER_PATH, CHARS_TO_REMOVE)
    quantang = load_quantang_sentences(QUANTANG_HTML_PATH, CHARS_TO_REMOVE)

    # 2. åˆ†è©
    ws = CkipWordSegmenter(device=0, model="bert-base")
    origin_tokens = segment_in_batches(origin, ws, text_type="åŸå§‹æ–‡æœ¬")
    compared_tokens = segment_in_batches(compared, ws, text_type="æ¯”å°æ–‡æœ¬")
    _ = segment_in_batches(quantang, ws, text_type="å…¨å”æ–‡")

    # 3. éæ¿¾æ¥µçŸ­å¥å­
    filt = [(r, toks) for r, toks in zip(origin, origin_tokens) if len(toks) >= MIN_TOKEN_LEN]
    origin, origin_tokens = zip(*filt)
    origin, origin_tokens = list(origin), list(origin_tokens)
    filt_c = [(c, toks) for c, toks in zip(compared, compared_tokens) if len(toks) >= MIN_TOKEN_LEN]
    compared, compared_tokens = list(zip(*filt_c)) if filt_c else ([], [])
    compared, compared_tokens = list(compared), list(compared_tokens)

    # 4. Hashing Trick è¿‘ä¼¼éæ¿¾
    origin_hash = tokens_to_gpu_hash_matrix(origin_tokens)
    total = len(compared)
    matches = []
    with tqdm(total=total, desc="Hashing Jaccard åŒ¹é…", unit="å¥") as pbar:
        for st in range(0, total, BATCH_SIZE):
            ed = min(st+BATCH_SIZE, total)
            comp_hash = tokens_to_gpu_hash_matrix(compared_tokens[st:ed])
            approx_scores, idxs = batch_jaccard_gpu(comp_hash, origin_hash)
            approx_scores, idxs = cp.asnumpy(approx_scores), cp.asnumpy(idxs)
            for i, (s, oid) in enumerate(zip(approx_scores, idxs)):
                if s < JACCARD_THRESHOLD:
                    pbar.update(1)
                    continue
                # 5. ç²¾ç¢º Jaccard äºŒæ¬¡éæ¿¾
                a = set(compared_tokens[st+i])
                b = set(origin_tokens[oid])
                exact_s = exact_jaccard(a, b)
                if exact_s >= JACCARD_THRESHOLD:
                    od = origin[oid]
                    cd = compared[st+i]
                    matches.append({
                        **{k: od[k] for k in [
                            "article_num","author","article_title",
                            "paragraph_num","group_num","sentence_num","original"]},
                        "matched_file": cd["matched_file"],
                        "matched_index": cd["matched_index"],
                        "matched": cd["raw"],
                        "similarity": exact_s
                    })
                pbar.update(1)

    # 6. è¼¸å‡º
    matches = sorted(matches, key=lambda x: (
        int(x["article_num"]), int(x["paragraph_num"]),
        int(x["group_num"]), int(x["sentence_num"]) ))
    with OUTPUT_JSON_PATH.open("w", encoding="utf-8") as f:
        json.dump(matches, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    main()
